---
layout: single
title: "SSD: Single Shot MultiBox Detector 논문 요약"
toc: true
toc_sticky: true
category: Detection
---

# Abstract
본 연구는 single deep neural network로 object detection을 수행한다. SSD라는 접근 방식은 bounding box를 피처맵의 위치별로 종횡비와 배율이 다른 default box 집합으로 이산화하는 것이다. 네트워크는 default box내에 있는 각각의 카테고리에
대해 점수를 부여하고 객체의 형태에 맞도록 box를 조정한다. 또한, 다양한 resolution을 갖는 피처맵을 결합하여 object의 사이즈가 다양하더라도 예측을 잘 수행할 수 있게한다. SSD는 region proposals이나 features resampling 단계를 없애고 모든 계산을
단일 네트워크에서 캡슐화 하기때문에 object proposals이 필요한 다른 네트워크에 비해 간단하다. PASCAL VOC, COCO, ILSVRC datsets에서 실험한 결과, SSD는 region proposals이 필요한(RCNN 등) 다른 모델들에 비해 준수한 정확도를 가진다. VOC 2007
에서 300 x 300 이미지를 사용했을 때 74.3% mAP, 59FPS를 기록했고, 512 x 512 이미지에서는 76.9% mAP를 기록했다. 대회 SOTA인 Fast R-CNN의 성능보다 더 좋은 성능이다. 또한, 다른 single stage methods와 비교해도 적은 input size로 더 좋은 정확도를 가졌다.

<br><br>

# Details
## Introduction
- 최근 object detection 모델은 bounding box를 제안 받고, 각 bbox에 대해 pixel or features를 resampling하고, 좋은 분류기를 연결하는 등의 접근 방식을 조금 변형해서 속도와 성능을 높였다. Fast R-CNN이 대표적인 모델로써 PASCAL, COCO 등의 탐지 대회에서 좋은 성능을 가졌다.
- 이러한 접근 방식은 정확하긴하지만 계산 비용이 너무 크고 속도가 너무 느리다는 단점을 가지고 있다. 속도는 프레임당 초를 의미하는 SFP로 비교하는데 가장 빠르다는 Fast R-CNN조차 7 FPS를 가지고 있다. 현재는 속도를 얻는 대가로 정확도를 희생시키는게 전부인 추세다.
- 본 논문은 bounding box 가설을 위한 pixel or featrues resampling을 수행하지 않아 속도를 크게 향상 시킨 deep neural network 기반 object detection을 소개한다. 속도 향상을 위해 위의 작업을 제거한 것이 처음은 아니지만 몇가지 개선사항을 추가해 이전보다 정확도를 크게 높일 수 있었다.
- 개선 사항으로는 작은 convolution filter를 사용하여 bounding box 위치의 offset과 object categories를 예측하는데 이 filter는 분리되어서 각각 다른 종횡비로 탐지를 수행하고 분리된 filter를 통해 여러개의 피처맵을 결합하여 다양한 scale의 object도 잘 탐지할 수 있도록했다.
- 특히, 각각 다른 scale 정보를 가지고 예측을 수행하도록 multiple layer를 만들면 상대적으로 낮은 resolution input을 가지고도 높은 정확도와 빠른 속도를 낼 수 있었다.
- SSD 연구가 기여할 수 있는 부분은 다음과 같다.
  - Single shot detetcor의 이전 연구이자 SOTA였던 YOLO보다 빠르고, region proposals과 pooling을 사용한 다른 느린 모델(Fast R-CNN 포함)보다 더 정확한 single shot detector(SSD) for multiple categories를 소개한다.
  - SSD의 핵심은 피처맵에 적용된 작은 convolutional filter를 사용하는 fixed default box 집합을 통해 category scores와 bbox의 offsets을 예측하는 것이다.
  - 정확도를 높이기 위해 다양한 scale의 피처맵에서 다양한 scale의 예측을 수행하고 종횡비별로 분리된 예측을 수행했다.
  - 이러한 구조로 단일 네트워크를 구축하고 low resolution input에서도 높은 정확도를 낼 수 있었고 속도와 정확도의 trade-off도 개선시켰다.
  - 실험 파트에서는 속도와 정확도를 평가지표로 삼아 PASCAL, COCO, ILSVRC datasets에서 각각 다른 image size를 사용했을 때 결과가 어떻게 달라지는지, 다른 SOTA 모델들과 어떤 차이가 있는지 실험했다.

<br><br>

## The Single SHot Detector(SSD)
<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/591652b0-afd2-4b4c-8322-ac62acd483fb">
  </p>
</div>

<br>

위의 그림은 SSD의 프레임워크를 나타낸 것으로, SSD는 CNN 단계에서 각각의 다른 크기의 피처맵을 통해 위치마다 다른 종횡비를 가진 몇개의 default box들을 평가한다. 또한, default box의 예측값으로 default box의 shape offsets과 confidences를 출력한다.
예를 들어, 먼저 default box들과 정답 box를 비교할 때 Figure 1.을 보면 8x8 피처맵에서는 2개의 default box가 고양이를, 4x4 피처맵에서는 하나의 default box가 개를 예측했고, 8x8 피처맵에서는 상대적으로 크기가 큰 개에 대해서는 예측을 잘 수행하지 못했다. 이렇게 default box 중 물체가 존재한다고 판단한 box를
positive, 그 외의 박스는 negative라고 할 수 있다. 모델의 loss는 localization loss(confidence loss(ex. softmax) 포함)를 가중합하여 계산한다.

추가적으로, 피처맵의 크기가 작다는 것은 그만큼 큰 filter를 썼다는 것이기때문에 크기가 작은 피처맵에서는 큰 객체에 대한 탐지가 가능하다. SSD에서는 이처럼 피처맵의 크기를 layer마다 다르게해서 다양한 scale의 object를 잘 탐지하도록 한다.


### 1. Model
SSD는 CNN을 기반으로 고정된 크기의 bbox들을 처리하고, bbox 내 존재하는 object들의 존재확신도 점수를 매긴다. 마지막 detection에서는 NMS 기법을 통해 중복된 박스를 제거하여 객체당 하나의 박스만 존재하도록 한다. 네트워크 앞단에는 image classification에서
에서 높은 성능을 자랑했던 high quailty 구조를 사용(base network)하고 그 후 layer들을 추가하여 detection tasks를 수행하도록 한다.

<br>

**Multi-scale feature maps for detection**
- Base network에 CNN 구조를 연결한다. 새로 연결한 layers 에서는 피처맵의 사이즈를 점점 줄여나가면서 multiple scales에서 예측을 수행하도록 한다.
- 각각의 feature layer마다 다른 scale의 예측을 수행하게 된다.

<br>

**Convolutional perdictors for detection**
- 추가된 layer(아니면 base network에서 일부 layer를 더 활용해도됨)에서는 filter를 통해 각 층마다 고정된 detection predictions 집합(Figure 2.를 보면 conv 층 3x3 필터에 고정된 갯수의 default box가 생성된 것을 알 수 있음)을 생성할 수 있다.
- Layer에서는 mxnxp의 사이즈를 갖는 filter를 사용하고, 기본적으로는 detection을 위해 3x3xp filter를 통해 cagory scores와 default box coordinates를 도출한다.
- Bounding box의 offsets은 각 피처맵 위치에 존재하는 default box position과의 상대적 거리를 통해 계산된다(YOLO에서는 offsets을 구하기위해 CNN이 아닌 FC layer를 사용했음).

<br>

<div align="center">
  <p>
  <img width="700" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/446069e6-a3fe-4428-90c1-fd92d90ff52d">
  </p>
</div>

<br>

**Defalut boxes and aspect ratios**
- Default box들의 집합은 네트워크 상단에 있는 여러개의 피처맵 셀들에 연결된다. 이 box들은 피처맵 셀의 알맞은 위치에 고정된다. 그리고 각 피처맵 셀에서 default box의 모양을 기준으로 offsets을 예측하고, 각 box 내 object들의 물체 존재 확신도를 예측한다.
- 주어진 위치 k개에서 box들의 offsets 위치 4개와 c개의 클래스의 물체 존재 확신도 점수를 계산한다. 그 결과, mxn 피처맵에 대해 (c + 4)kmn의 결과가 도출된다.
- Default box는 Faster R-CNN의 anchor boxes와 비슷한 개념이지만 box마다 다양한 resolution을 적용했다는 점에서 차이가 존재한다.

<br>

### 2. Training
SSD와 다른 detector와의 region proposals 단계에서 가장 큰 차이점은 SSD는 ground truth 정보가 고정된 detector outputs 집합에 꼭 할당 되어야한다는 것이다. 이 방법은 YOLO, Faster R-CNN, MultiBox 등 다른 모델들에서도 사용된다.
이 정보가 한번 할당되면 loss function과 역전파 전 과정에 적용된다. 학습에는 default boxes의 갯수와 scales을 얼마나 정할 것인지와 hard negative mining, data augmentation 등이 포함된다.

<br>

**Matching strategy**
- 학습을 할 때는 어떤 default box가 ground truth와 관련이 있는지 파악하고, 이에 따라 네트워크를 훈련해야한다. 이는 gound truth box와 가장 많이 겹치는 default box를 선택한다. Multibox 모델과는 달리, 어떤 ground truth냐와는 상관없이 jaccard overlap(IOU)이 threshold인 0.5보다 큰 default box라면 모두 정답과 연관이 있다고 판단한다.
- 이 방법은 가장 overlap이 큰 box를 하나만 남겨두는 것보다 오히려 다양한 scale을 가진 default box들을 통해 예측을 수행하여 learning problem을 단순화시킨다.

<br>

**Training objective**
- SSD의 손실 함수는 MultiveBox 모델과 유사하지만 multiple object categories를 처리하도록 수정되었다.
- $x_{ij}^p = {1,0}$를 p라는 category에 대해 i번째 default box와 j번째 ground truth가 연결되었는지를 나타낸다고 하자.
- default box가 여러개있기때문에 ground box와 일치된다고 판단된 경우가 많다면 $\Sigma_{i} x_{ij}^p \geq 1$ 이 될 수 있다.
- 전체 손실함수는 다음과 같이 정의된다.

$$ L(x,c,l,g) = \frac{1}{N} (L_{conf}(x,c) + \alpha L_{loc}(x,l,g))$$

- N은 matching된 default box의 갯수를 의미하고 N = 0이면, loss를 0으로 설정한다.
- Localization loss는 predicted box($l$)과 ground truth box($g$)간의 Smooth L1 loss를 적용한다.
- Faster-RCNN과 비슷하게 default box($d)에 대해 center($cx, cy$)와 $w,h$를 offsets으로 사용한다.

<br>

<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/2de4931f-5d53-426b-9e70-57424ef50681">
  </p>
</div>

<br>

- Confidence loss는 multiple class confidences($c$)에 대한 softmax를 활용한다.

<br>

<div align="center">
  <p>
  <img width="600" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/a8f7a2b1-9fa8-445a-92ae-af15c268d30c">
  </p>
</div>

<br>

- Cross validation을 통해 가중치항 $\alpha$는 1로 설정한다.
- 정리하자면 손실함수는 ground truth와 매칭되는 default box들의 손실로 계산되는데
  - localization에 대한 손실을 offsets으로 계산하고
  - box내 예측한 여러 class에 대한 손실을 softmax로 계산하여 두 손실을 합치는 방법을 사용한다.
  - 합쳐진 두 손실은 matching된 box의 수로 나눠 최종 loss를 구한다.

<br>

**Choosing scales and aspect ratios for default boxes**
- Object의 다양한 scale을 처리하기위해 어떤 모델은 이미지 사이즈를 직접적으로 처리하는 방법을 사용하지만 SSD에서는 하나의 단일 네트워크에서 layer마다 다른 scale을 갖는 피처맵을 활용하고 이를 공유한다.
- 이전 연구들에 따라 층이 얕을수록 더 좁고 세밀한 정보를 파악할 수 있기때문에 얕은층과 깊은층의 피처맵들을 detection에 사용했다. 위의 Figure 1.에서 8x8과 4x4 피처맵이 크 예시이다.
- 층마다 피처맵의 크기가 다르다면 층마다 receptive field가 달라지게될텐데 다행히 SSD의 default box는 꼭 실제 receptive fields와 box의 크기를 일치시킬 필요는 없다. 피처맵이 특정 objects의 scale에 맞게 학습되도록 default box의 크기를 조정할 수 있기때문이다. 특정 에측을 위해 m개의 피처맵이 필요하고 각 층을 k라고 한다면 default box의 scale을 다음과 같이 계산할 수 있다.

<br>

<div align="center">
  <p>
  <img width="600" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/744ddda2-6713-4fbc-ba5d-b46d304cd090">
  </p>
</div>

<br>

- $S_{min}$이 0.2, $S_{max}$가 0.9라는 것은 가장 낮은 층의 scale이 0.2과 가장 높은층의 scale이 0.9라는 의미이다. 모든 층의 scale은 이 안에 있다.
- 다양한 default box의 aspect ratio는 $a_r \in [1,2,3,1/2,1/3]$ 으로 정의하고(1은 가로세로 비율이 같은 것, 2는 세로가 가로보다 2배 큰 것, 1/2는 가로가 세로보다 2배 큰 것) box마다 width는 $w_a^k = s_k \sqrt{a_r}$, height는 $h_a^k = s_k \sqrt{a_r}$로 계산한다.
- 이러한 방법으로 default box의 크기를 유연하게 조절하며 다양한 scale에서 bounding box를 그리고 추출된 수많은 피처맵들을 통합기때문에 특정 피처맵에서 객체를 발견하지 못하더라도 다른 피처맵에서 객체를 발견하여 예측에 큰 도움을 준다.

<br>

**Hard negative mining**
- Matching 단계 이후에는 전경보다 배경이 훨씬 많기때문에 많은 default box들이 negative일 것이다.
- 모든 negative examples을 쓰는 대신 각 default box마다 confidence loss를 정렬하고 가장 좋은 box를 선정해서 negative와 positive의 비율이 최대 3:1 정도가 되도록 한다.
- 이를 통해 최적화가 더 빨라지고 안정적인 학습이 이루어질 수 있다.

<br>

**Data augmentation**
- 모델을 더 robust하게 만들기 위해서 input object의 크기나 형태를 변형하고 다음과 같은 조건에 option으로 input image를 랜덤하게 샘플링했다.
  - Original input image
  - IOU가 0.1, 0.3, 0.5, 0.7, 0.9인 객체들을 패치로 샘플링
  - 랜덤하게 패치로 샘플링
- patch의 크기는 원본 이미지의 0.1과 1사이로 하고 종횡비는 0.5와 2사이로 한다. 샘플링된 패치에도 ground truth box가 겹쳐있다면 예측에 사용한다.
- 샘플링 단계 이후에는 샘플된 패치들을 고정된 크기로 리사이징하고 0.5의 확률로 수평변환시킨다. 추가로 photo-metric distortions을 적용한다.

<br><br>

## Experimental Results
**Base network**
- 사전훈련 모델로 VGG16을 사용하고 DeepLab-LargeFOV처럼 fc6와 fc7 layer를 convolutional layers로 변환시켰다. 또한, Pool5의 2x2(s2)를 3x3(s1)으로 바꾸고 atrous algorithm을 사용했다. 모든 dropout layers와 fc8 층을 제거했다.
- Fine tuning에는 optimizer SGD, learning rate 0.001, momentum 0.9, weight decay 0.0005, batch size 32를 사용했다.

<br>

<div align="center">
  <p>
  <img width="700" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/446069e6-a3fe-4428-90c1-fd92d90ff52d">
  </p>
</div>

<br>

### 1. PASCAL VOC 2007
<br>

<div align="center">
  <p>
  <img width="700" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/6f2abeb2-41d2-45a9-98ba-e4209b023ea1">
  </p>
</div>

<br>

- Fast-RCNN, Faster R-CNN과 성능을 비교했다.
- SSD300은 이미지 사이즈를 300x300으로 조정한 모델이고 conv4_3, conv7(fc7), conv8_2, conv9_2, conv10_2, conv11_2에서 location과 confidences를 계산했다. Conv4_3의 default box scale은 0.1로 설정했고, 새로 추가된 convolutional layer의 파라미터는 xavier 방법으로 초기화했다.
- Conv4_3, conv10_2, conv11_2에는 각각의 피처맵마다 4개의 default box를 사용했고, 다른 모든 layers에는 6개의 default box를 사용했다. 특히, conv4_3은 다른 layers들과 다른 feature scale을 가지고 있어서 L2 정규화로 feature norm을 20으로 조정하고 역전파 과정에서 scale을 학습했다.
- 위의 Table 1을 보면 SSD 300 모델이 Fast R-CNN보다 높은 정확도를 가진다는 것을 알 수 있고, SSD512로 이미지를 키웠을 때는 Faster R-CNN보다 높은 정확도를 기록했다. COCO 데이터를 추가하는 등 더 많은 data로 학습을 진행했을 때는 SSD512 모델에서 81.6% mAP를 기록했다.

<br>

<div align="center">
  <p>
  <img width="700" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/12633d38-0345-4068-b1b8-10399f47e50a">
  </p>
</div>

<br>

- Figure 3.을 보면 white area(correct)가 넓은 것을 보아 SSD model이 다양한 category에 대해 예측을 잘 수행한 것을 알 수 있다. Recall도 85~90% 정도였고 IOU를 0.1로 낮추면 더 높은 Recall을 보인다.
- SSD는 R-CNN처럼 두 가지의 step을 걸쳐 localization과 classification이 다르게 이루어지지 않고 하나의 네트워크에서 직접 bbox와 class를 예측하기때문에 R-CNN보다 localization error가 작다.
- 하지만, 다양한 category들의 정보를 공유하기때문에 비슷한 objects에 대해서는 예측을 잘 수행하지 못한다.

<br>

<div align="center">
  <p>
  <img width="800" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/d58bcee4-e629-4c27-876e-48b31017c8f4">
  </p>
</div>

<br>

- Figure 4.를 보면 SSD는 bounding box size에 굉장히 민감한 것을 알 수 있고 큰 box보다 작은 box 때문에 성능이 더 하락했다. 작은 객체는 얕은 layer에서 정보를 얻어내기가 힘들기때문에 어찌보면 당연한 결과다.
- 300에서 512로 사이즈를 키워도 small object에 대한 탐지가 어려웠지만 그럼에도 다양한 scale의 박스와 피처맵을 통해 강건한 모델을 만들었다는 점에서 긍정적인 실험이었다.

<br>

### 2. Model analysis
SSD를 더 잘 이해하기위해 각각의 component들이 성능에 어떻게 영향을 미치는지 살펴보자. components의 영향을 잘 파악하기위해 모든 실험에 300 x 300사이즈의 이미지를 사용했다.

<br>

<div align="center">
  <p>
  <img width="700" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/9d2d77d4-c332-4745-8317-fde689927ba5">
  </p>
</div>

<br>

**Data augmentation is crucial**
- Fast, Faster R-CNN은 원본이미지와 수평변환을 학습에 사용했다. SSD는 YOLO와 비슷한 방법으로 sampling을 수행해 성능을 향상시켰다.
- SSD의 샘플링 기법이 Fast, Fater R-CNN에도 효율적일진 모르겠지만 object의 robust에 중요한 단계인 classification 중간에 pooling이 있기때문에 아마 효과가 좋진않을 것이다.

<br>

**More default box shapes is better**
- SSD는 location마다 보통 6개의 default box를 사용했다. 만약 aspect ratios $\frac{1}{3}, $3$으로 boxes를 제거하면 성능이 하락했고, $\frac{1}{2}, $2$도 마찬가지였다.
- 다양한 크기의 default boxes를 사용하는 것이 성능 향상에 도움이 된다.

<br>

**Atrous is faster**
- DeepLab 모델처럼 VGG16에 atrous convolution을 적용했다.
- 만약, VGG16을 그대로 사용해서 pool5(2x2, s2) 사용, fc6, fc7의 파라미터에 subsampling 적용 X, conv5_3을 추가하는 방법을 사용하면 정확도는 같은데 속도는 20% 느려졌다.

<br>

**Multiple output layers at different resolutions is better**
<br>

<div align="center">
  <p>
  <img width="700" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/a8746e22-5b66-413b-88f6-1527259cc8ed">
  </p>
</div>

<br>

- SSD는 다양한 output layers에서 다양한 scale의 default box를 사용하는 것이 핵심이기때문에 layers를 점점 줄여가면서 이 효과를 검증해봤다.
- 정확한 비교를 위해 layer를 제거할 때마다 default box tiling(각 픽셀마다 default box를 잘 배열하는 것)을 조정하여 총 상자 수를 원본과 비슷한 8732개로 유지했다(하지만, 모든 실험에서 tiling을 한 것은 아님). 남은 layer에 box scale을 더 많이 쌓고 필요한 경우 box scale을 조정했다. layer에 box를 쌓을 때 box가 이미지 경계에 있는 경우가 많으므로 주의하며 쌓아야한다.
- Table 3.는 layer가 적을수록 성능이 떨어지는 것을 보여준다.
- Faster R-CNN처럼 경계에 있는 box를 무시한채 예측을 진행했더니 흥미로운 사실을 발견했다. 만약 11_2, 10_2와 같은 깊은 layer를 사용한다면 성능이 크게 떨어진다는 것이다. 아마 이미지 경계에 있는 box들이 예측에 포함되지않아 큰 객체를 감쌀 수 있는 큰 box가 충분치 않아서일 것이다.
- 또한, conv7만을 사용했을 때 최악의 성능이 나왔는데 이것으로 다양한 scale의 box를 여러 layer에 적절히 분배하는 것이 성능에 큰 영향을 끼친다는 것을 알 수 있다.
- 
- SSD는 lower resolution(300x300) input image를 사용했는데도 Faster R-CNN과 비슷한 정확도를 가졌다.


<br>

### 

<br><br>

## 

<br><br>

## Experiments


<br>

### 
<br>

### 
<br>

### 
<br>

### 
<br>

### 
<br><br>

## 

<br><br>

# Conclusion
- 

<br><br>

# 개인적인 생각
- 

<br><br>

# 이미지 출처
- [SSD: Single Shot MultiBox Detector Paper](https://arxiv.org/pdf/1512.02325v5.pdf)
