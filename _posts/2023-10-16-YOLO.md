---
layout: single
title: "YOLO : You Only Look Once: Unified, Real-Time Object Detection 논문 요약"
toc: true
toc_sticky: true
category: Detection
---

# Abstract
객체 탐지의 이전 연구들에서는 분류기에서도 detection을 수행할 수 있도록 했다. 본 연구에서는 하나의 단일 신경망을 사용해서 여러 bounding box와 class probabilities를 예측하는 객체 탐지를 
regression 문제로 취급하여 해결했다. 본 연구의 YOLO 모델은 객체 탐지 파이프라인 전체가 하나의 네트워크로 이루어져있어서 실시간 이미지를 초당 45프레임으로 처리할만큼 굉장히 빠르다. 다른 네트워크와
비교했을 때 localization에 대한 오류가 존재하긴하지만 배경에 대해서는 예측을 잘 수행하고 RCNN, DPM보다 다른 도메인에 대해서도 일반화가 잘 된 예측을 수행한다.

<br><br>

# Details
## 도입부
- 최근 detection networks들은 분류기가 탐지를 수행할 수 있도록 변형한다. 탐지 역할을 하기 위해서 분류기는 다양한 위치와 크기의 이미지들에서 객체를 탐지하고 평가한다. DPM 형태의 모델에서는
sliding window 개념을 활용하여 전체 이미지에 대해 균일한 간격으로 분류를 수행한다.
- 보다 최근인 RCNN에서는 이미지에 대해서 물체가 있을법한 곳에 bounding box를 생성하고 분류기는 이 bounding box에 대해 분류를 수행한다. 분류 후에는 bounding box 내의 객체가 중복되면
이를 제거하고, 다시 class에 대해 분류를 수행했다. RCNN의 경우 각각의 tasks들이 별개로 이루어져있기때문에 복잡하고 예측에 많은 시간이 소요된다는 단점을 가지고있다.
- YOLO(You Only Look Once)에서는 단일 네트워크로 CNN에서 여러 bbox와 각 bbox 마다 존재하는 class의 확률을 예측한다. 아래 Figure 1. 이미지를 보면, YOLO는 이미지를 고정된 448 x 448
사이즈로 리사이징 하고, CNN을 통해 이미지의 특징을 추출한 뒤 NMS 기법으로 객체당 하나의 bbox를 남기고, bbox 마다 thresholds(물체 존재확신도)를 기반으로 예측을 수행한다. 
- YOLO의 장점은 첫째, YOLO는 detection을 하나의 regression problem으로 취급하기때문에 파이프라인이 복잡하지 않다. 속도가 매우 빠르면서 다른 실시간 탐지 시스템에 비해 두배 이상의 mAP를 기록했다.
- 둘째, YOLO는 sliding window나 region proposals 기법과 달리 훈련 및 테스트 시간 동안 전체이미지를 보기때문에 클래스에 대한 context 정보다 class의 형태 등과 같은 정보를 암시적으로 인코딩할 수 있다.
Fast,RCNN의 경우 large context 정보가 제대로 반영되지 않기때문에 background patches에 예측력이 좋지않다. YOLO는 Fast-RCNN의 절반 정도의 background 예측 에러를 갖고있다.
- 셋째, YOLO는 자연 이미지를 학습했고, 예술 이미지에 대해 test했기 때문에 일반화 능력이 뛰어나다. DPM, Fast-RCNN보다 새로운 도메인에 대해서도 예측을 잘 수행했다.
- 속도가 매우 빠르지만 아직 localization과 작은 객체 탐지에 대해 어려움을 겪고 있고 여러 실험을 통해 속도와 정확도간의 trade-off에 대해 살펴본다.

<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/8cb69ee4-8331-4954-9cae-311d0cd17261">
  </p>
</div>

<br><br>

## Unified Detection
YOLO network는 한 이미지에 존재하는 모든 클래스에 걸겨 모든 bbox를 동시에 예측한다. 즉, 네트워크는 전체 이미지와 이미지에 포함된 객체를 전역적으로 예측한다는 것이다. YOLO는 이미지를 SxS의
그리드로 나누는데 만약 객체의 중심이 그리셀 내에 있다면 해당 그리드셀에서는 반드시 객체가 탐지되어야 할 것이다. 각각의 그리드 셀마다 B개의 bbox와 bbox 내의 confidence score(물체 존재 확신도)를 예측한다. 이를 수식화하면 $Pr(Object) * IOU(truth|pred)$로 표현한다. 만약, 그리드셀 내에 객체가 없다면 confidence score는 0이 되어야한다. 물체가 존재한다면 confidence score는 IOU와 같아지길 원한다.

Bounding box는 x,y,w,h,confidence score 5개의 예측값을 가진다. x,y는 grid cell 내에서의 중심점 좌표고 w,h는 전체 이미지 내의 넓이와 높이다. Confidence score는 예측 box와 어떤 groun truth box간의 IOU를 대변한다.

또한, 각각의 그리드 셀은 C(conditonal class probabilities)라는 클래스 확률을 예측한다($Pr(Class_i|Object)$). 이 조건부 확률은 그리드 셀안에 객체가 있다는 조건 하에 해당 객체가 어뜬 class인지에 대한
확률이다. 그리드 셀 내의 Bounding box의 갯수와 상관없이 한 그리드 셀마다 하나의 class probabilities만 예측한다.

Test 단계에서는 C와 cofidence score를 곱해 box내에 특정 class가 존재할 확률과 bounding box가 얼마나 이 객체에 맞게 잘 형성되었는지를 파악한다.

$$Pr(Class_i|Object) * Pr(Object) * IOU(truth|pred) = Pr(Class_i) * IOU(truth|pred)$$

아래 Figure 2.에서 YOLO가 어떻게 detection을 수행하는지 잘 설명했다. Box당 4개의 좌표와 물체 존재 확신도, class probobilities가 필요하고 이것을 그리드마다 수행하게되므로 예측에는 총 S x S x (5B + C)의 텐서가 필요하다.

<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/7141e759-c21c-408e-abe5-d8372502f2b4">
  </p>
</div>

<br><br>

### 1. Network Design
- PASCAL VOC 데이터셋에서 평가를 진행한다.
- CNN에서 이미지의 특징을 추출하고 FC층에서 class 확률과 bbox 좌표를 예측한다.
- ImangeNet 데이터를 활용하여 classification task로 pretrain했고, detection에서는 224x224크기의 이미지를 두배로 늘려서 사용했다.
- 아키텍처는 GoogLeNet과 유사한데 인셉션 모듈대신 1x1, 3x3 합성곱을 사용했다(DarkNet). 
- Fast YOLO는 24 layer 대신 9 layer만 사용해서 속도를 빠르게 했다.

<br> 
<div align="center">
  <p>
  <img width="700" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/616adf0e-4dd2-481f-ab47-a25f97697bd8">
  </p>
</div>

<br>

### 2. Training
- 사전훈련한 모델은 ImageNet 2012에서 GoogLeNet과 비슷한 수준의 성능을 기록했고, 모든 훈련과 추론에 DarkNet 프레임워크를 사용했다.
- 객체 탐지를 수행하기 위해 4개의 convolutional layer와 랜덤하게 가중치가 초기화된 2개의 완전결합층을 추가해서 모델을 조금 수정했다. 또한, detection에는 더 세밀한 작업이 필요해 resolution을 2배로 높였다(448 x 448).
- 최종 layer에서는 class 확률과 bbox 좌표를 예측하는데 bbox는 w,h를 정규화해서 0과 1사이가 되도록했고, x,y 좌표도 특정 그리드 셀 위의 오프셋으로 변환하여 0과 1사이로 변환했다.
- 최종 layer만 linear activation을 사용했고 모든 다른 layer에서는 Leaky ReLU를 사용했다.

<br> 
<div align="center">
  <p>
  <img width="300" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/3210764f-0ce3-48a4-acce-6816e460c4fe">
  </p>
</div>

<br>

- MSE를 사용하면, 손실함수가 간단하지만 mAP를 극대화하려는 목표와는 거리가 있다. 또한, grid cell에 물체가 포함되지 않은 경우가 많다면 물체가 있는데도 confidence score가 0으로 되어 예측 자체가
수행되지 않은 경우가 생길 수 있기때문에 학습이 불안정할 수도 있다.
- 이를 해결하기 위해, bbox 좌표 예측의 손실을 높이고, 객체가 포함되지 않은 box에 대한 confidence 예측 손실을 줄이는 방법을 사용했다. localization과 classification 중 localization의 가중치를
더 증가시키고 객체가 있는 confidence loss의 가중치를 없는 가중치보다 더 증가시킨 방법이다(객체가 없는 경우가 훨씬 많기때문에 객체가 있을 때의 loss가 더 중요함). 이는 $\lambda_{coord} = 5$, $\lambda_{noobj} = .5$를 설정하여 해결할 수 있다.
- MSE는 bbox가 크든, 작든 동일한 가중치를 부여한다는 단점이 있다. 오차의 관점에서 작은 bbox가 큰 bbox보다 편차에 민감하다. 예를 들어, 큰 객체를 감싸고 있는 bbox가 0.5 움직여도 여전히 객체를 감싸고 있을 수도 있지만, 작은 객체를 감싸고 있는 bbox가 0.5 움직이면 객체에서 벗어날 수도 있기때문이다.
따라서 큰 bbox의 loss를 작은 bbox에 그대로 반영한다면 전체 loss가 커질 수도 있다. 이 문제를 해결하기 위해 넓이와 높이를 직접적으로 에측하지 않고, 제곱근을 예측하는 방법을 사용했다. 제곱근을 예측하면 box의 크기가 클수록 증가율이 낮아지기때문에 bbox가 커서 loss가 커지더라도 제곱근으로 인해 작은 bbox가 큰 영향을 받지 않게되고 loss를 줄일 수 있다.
Box가 클수록 증가율이 작아져 IOU에 적은 영향을 끼치게된다.
- 위의 손실함수 관련 내용을 수식으로 표현하면 다음과 같다.

<br> 
<div align="center">
  <p>
  <img width="400" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/488823e2-1e29-445f-a8f1-07ce68f27c5b">
  </p>
</div>

<br>

- $1_{ij}^{obj}$ 는 grid cell내에 class가 존재한다는 것을 의미하고 존재하면 1, 아니면 0($1_{ij}^{noobj}$)으로 표현한다. $1_{ij}^{obj}$ 는 grid cell $i$의 $j$번째 bbox predictor가 사용되는지의 여부이다. 위의 수식을 5단계로 표현하면
  - 먼저, 객체가 존재하고 그리드 셀 $i$의 bbox predictor $j$에 대해 x,y loss를 구한다.
  - 두번째, 객체가 존재하고 그리드 셀 $i$의 bbox predictor $j$에 대해 w,h loss를 구한다.(큰 bbox에 증가율이 커지지 않도록 제곱근을 예측)
  - 세번째, 객체가 존재하고 그리드 셀 $i$의 bbox predictor $j$에 대해 confidence loss를 구한다(물체가 존재하기때문에 $C_i=1$)
  - 네번째, 객체가 존재하지않을때 그리드 셀 $i$의 bbox predictor $j$에 대해 confidence loss를 구한다.($C_i=0$)
  - 다섯번째, 객체가 존재하고 그리드 셀 $i$의 bbox predictor $j$에 대해 class probabilities loss를 구한다.(class가 맞으면 $p_i(C)=1$ 아니면 0)
  - $\lambda_{coord}$ x,y,w,h의 좌표 loss와 다른 loss간의 밸런스를 위한 parameter.
  - $\lambda_{noobj}$ 객체가 있는 box와 없는 box의 loss 간의 밸런스를 위한 parameter.

- 위의 모든 과정을 다 더해서 손실함수를 만들고 모델을 최적화시킨다.
- 과적합을 막기위해서 dropout(0.5), 원본 이미지의 크기보다 최대 20%까지 랜덤하게 scaling & translation 적용, HSV factor 조절의 data augmentation을 사용했다.

<br>

### 3. Inference
- Test 시에는 PASCAL VOC에서 이미지당 98개의 bounding box가 그려졌고 각 박스에 대해 class를 예측했다.
- 물체가 너무 크거나 물체가 여러 셀의 경계 근처에 있다면 주변의 다른 셀들의 정보를 참고해야 원활한 localized가 수행될 수 있는데 이러한 경우 하나의 객체가 여러 셀에서 발견되는 multiple detection 문제가 생길 수 있다.
NMS를 통해 하나의 객체에 하나의 bounding box만 생기도록 했고 RCNN이나 DPM처럼 NMS가 매우 중요하진않지만 이를 사용했을 때 mAP가 조금 향상되었다.

<br>

### 4. Limitations of YOLO
- YOLO는 각 그리드 셀마다 두개의 bounding box가 그려지고, 각 셀은 오직 하나의 class로만 예측이되어야하기 떄문에 공간적이 제약이 생긴다. 이런 공간적인 제약은 근처에 있는 한 셀에 여러 objects가 있는 경우 모든 objects를 잘 탐지하지 못하고, 특히 크기가 작은 물체를 탐지하는데 어려움을 겪게한다.
- Data로부터 bounding box를 그려내기때문에 가로세로 비율이나 형태가 익숙하지 않다면 이를 잘 예측하는데 어려움을 겪는다. 또한, 여러 층을 거치며 downsampling된 features를 사용하기때문에 bbox를 예측하는 단계에서는 input image의 정보가 많이 선명하진 않을 것이다.
- Detection performance를 위해 loss function을 정의했지만, 작은 bbox나 큰 bbox의 loss function을 결국 동일하게 가져갔고 그 결과 큰 bbox보다 작은 bbox가 IOU에 많은 영향을 미쳤다.
- Error의 가장 큰 문제는 localization이다.

<br><br>

## Comparison to Other Detection System
YOLO detection system이 다른 systems들과 어떤 공통점 혹은 차별점을 가지는지 살펴보자.

**Deformable parts models**
- DPM은 sliding window 개념을 활용하여 객체를 탐지한다.
- DPM은 각각 분리된 형태로 파이프라인을 구축해 features를 추출하고, regions에서 분류를 수행하고, 가장 점수가 높은 regions에서 bbox를 예측한다.
- YOLO는 이 모든 과정을 하나로 통합했다는 점에서 DPM과 차이가 있다. 통합된 아키텍처로 DPM보다 더 빠르고 정확한 모델을 만들었다.

<br>

**R-CNN**
- R-CNN은 sliding window 대신 selective search 알고리즘으로 다양한 region proposals을 생성하고, CNN으로 features를 추출, SVM으로 분류를 수행, linear model로 bbox 예측, 중복된 bbox는 NMS로 제거하는 기법들을 사용했다.
- 굉장히 복잡한 파이프라인이고 각각의 tasks가 모두 독립적으로 이루어져 results를 출력하는 속도가 매우 느리다는 단점을 가지고있다.
- YOLO는 potential bbox를 제안받고, CNN을 통해 features를 추출한다는 점은 동일하지만 selective search 아닌 공간적 제약을 가진 grid cell로 region proposals을 수행한다는 점에서 차이가 존재한다.
- 또한, 2000개의 bbox가 생성되는 R-CNN에 비해 98개의 bbox만 생성되고, 각각의 components를 하나로 통합했다는 점에서 R-CNN과 차이가 있다.

<br>

**Other Fast Detectors**
- DPM과 R-CNN 모두 각각의 components를 개선시켜 속도와 성능을 높였지만, YOLO는 여전히 애초에 하나의 pipeline에서 속도가 빠르도록 네트워크를 구축했다는 점에서 차별점이 존재한다.
- YOLO는 general purpose detector로 다양한 객체들을 동시에 예측할 수 있다.
- 이외에도 YOLO는 다른 여러가지 detector system보다 빠르고 한 이미지 내에서 single 뿐만 아니라 multiple objects도 잘 탐지할 수 있는 모델이다.

<br><br>

## Experiments
YOLO와 다른 real-time detection systems를 비교하기 위해 VOC 2007 데이터셋을 Fast-RCNN과 비교했다. Error를 계산하는 방법이 다르기때문에 Fast-RCNN을 rescore했고, background
false positives 에러를 감소해서 기존보다 Fast-RCNN의 성능을 높였다. 또한, VOC 2012의 SOTA mAP와 비교했고, 최종적으로 YOLO의 일반화능력을 평가하기 위해 새로운 도메인인 예술에서 다른 시스템과 비교해보았다.

<br>

### 1. Comparison to Other Real-Time Systems
- 많은 연구들이 networks를 빠르게 만드는데 초점을 맞추고있다. 하지만 실제로 초당 30프레임 이상으로 실행되는 시스템은 DPM 모델밖에 없다. 30Hz or 100Hz로 실행되는 DPM의 GPU와 YOLO를 비교했다.
- Fast YOLO는 현존하는 가장 빠른 물체 감지기이고 52.7%의 mAP로 이전보다 2배 이상 정확한 모델이다. YOLO 또한, mAP 63.4%까지 성능을 끌어올렸다.
- VGG-16을 사용해서 YOLO를 훈련시키면 모델이 정확하지만 속도가 기존보다 떨어졌다. 이는 VGG를 기반으로 한 다른 모델들과 비교하기에는 유용하지만 YOLO보다 많이 느려서 VGG로 훈련시키지 않은 원래 YOLO로 비교를 진행했다.
- DPM은 mAP의 큰 희생없이 속도를 효과적으로 높였으나 여전히 성능은 2배 이상 떨어진다. 특히, 딥러닝을 통한 접근에도 성능이 높지 않다는 점에서 한계를 가지고있다.
- R-CNN에서 R을 빼면 selective search가 static bonding box proposals로 바껴서 R-CNN보다 훨씬 빠른 모델이 만들어진다. R-CNN은 여전히 region proposals에 의존하며 좋은 proposals이 없다면 높은 정확도를 낼 수 없다.
- Fast-RCNN은 R-CNN보다 속도가 빨라졌지만 여전히 selective search에 의존하며 이미지당 2초의 proposals 시간이 소요된다. 따라서, mAP는 높지만 0.5fps라서 실시간으로 취급하긴 어렵다.
- 가장 최근 모델인 Faster R-CNN은 selective search를 neural network로 대체해서 성능과 속도에 큰 향상을 이루어냈다. 테스트 결과, 가정 정확도가 높은 모델은 7fps, 더 작은 대신 덜 정확한 모델은 18fps로 실행됐다.
Fast R-CNN에 다른 여러가지 모델을 훈련시키면 정확도에 향상에 비해 속도가 YOLO보다 크게 느려졌다.

<br> 
<div align="center">
  <p>
  <img width="400" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/07ab98ed-c07f-460c-a5de-bd2d4ff83e2f">
  </p>
</div>

<br>

### 2. VOC 2007 Analysis
- 현존하는 모델 중 PASCAL에서 가장 좋은 성능을 가지고 있는 Fas R-CNN 모델과 성능을 비교했다. Hoiem의 방법론을 사용했고 각 class에 대해 top N predictions(클래스마다 가장 잘맞춘 N개의 경우를 평균내어 에러를 비교)을 확인했다. 각각의 prediction은 correct or 다음과 같은 type of error로 구분된다.
  - Correct: correct class and IOU > 0.5
  - Localization: correct class, 0.1 < IOU < 0.5
  - Similar: class is similar, IOU > 0.1
  - Other: class is wrong, IOU > 0.1
  - Backgruond: IOU < 0.1 for any object
- 아래 Figure .4를 통해 YOLO는 localization error에 다른 에러를 합친 것보다 더 클만큼 어려움을 겪고있다는 것을 알 수 있다.
- Fast R-CNN은 localization error는 작지만 backgound에 대해 예측을 잘 수행하지 못하고있다. 특히, 모델이 object라고 예측했는데 실제로는 배경이었던 false positives에 포함된 objects가 하나도 없었다. 즉, 배경을 제대로 맞추지 못하는 것이다.

<br>

### 3. Combining Fast R-CNN and YOLO
- YOLO가 Fast R-CNN보다 배경을 잘 예측하기때문에 이 둘을 조합하여 성능을 향상시켰다. R-CNN이 예측한 bbox에 대해 YOLO가 유사한 박스를 예측하면 YOLO가 미리 지정한 확률과 두 박스 간의 겹침의 정도에 따라 해당 예측에 boost를 준다.
- 그 결과, YOLO와 결합했을 때 Fast R-CNN은 기존보다 3.2% 증가한 75%의 mAP를 달성했다.
- 아쉽게도 이 결합 모델은 각 모델을 개별적으로 실행한 다음 결과를 합치기때문에 YOLO의 속도 이점을 누릴 순 없지만, Fast R-CNN 원래 속도에 비해 계산 시간이 크게 추가되진 않았다.

<br> 
<div align="center">
  <p>
  <img width="400" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/c316d60a-8fe9-4db0-95ed-df8dbff25193">
  </p>
</div>

<br>

### 4. VOC 2012 Results
- VOC 2012 testsets에서 YOLO는 57.9% mAP를 기록했다. 이는 VGG-16을 사용한 original R-CNN 모델과 비슷한 성능이다. 특히, 병, 모니터 등과 같은 작은 물체를 잘 예측하지 못했다.
- 하지만, 고양이와 기차 등 다른 카테고리에서는 YOLO가 더 높은 성능을 기록했다. Fast R-CNN과 YOLO 결합 모델은 70.7% mAP를 기록하며 최종 5위를 기록했다.

<br>

### 5. Generalizability: Person Detection in Artwork
- 현실에서는 모델이 접해보지 못한 수많은 데이터가 존재한다. 따라서, YOLO의 일반화 능력을 평가하기 위해 Picasso Dataset과 People-Art Datset을 사용하여 예술 작품 속에서 사람을 탐지하는 test를 진행했다.
- 성능은 사람만 탐지할 것이기때문에 people class에 대한 average precision을 지표로 사용했다. 모든 모델은 VOC 2007의 사람 데이터로 학습했고 Picasso model은 VOC 2012로, People-Art 모델은 VOC 2010으로 학습했다.
- R-CNN은 VOC 2007에서 AP가 높았지만, artwork에서는 AP가 크게 떨어졌다. 분류기 단계에서 작은 regions을 보고 좋은 proposals을 수행해야하기때문에 어려울 것이다.
- DPM은 artwork에서도 비슷한 AP를 가졌다. Object에 대해 공간 정보를 잘 간직한 모델이기때문에 R-CNN보다 감소량이 적을 것이다. 하지만, 애초에 AP 자체가 높지않다는 것이 문제다.
- YOLO는 VOC 2007에서 성능이 우수할 뿐만 아니라 artwork에서도 성능 저하가 작다. YOLO도 DPM처럼 객체의 모양이나 공간 정보를 잘 간직하고, artwork는 일반 이미지와 다르지만 객체의 크기와 모양이 비슷했기때문에 좋은 성능을 냈을 것으로 추측한다.

<br><br>

## Real-Time Detection In The Wild 
- 웹캠을 통해 야생에서 객체 탐지를 수행했을 때 YOLO는 객체가 움직이거나 모양이 변할 때 이를 감지했다.

<br><br>

# Conclusion
- YOLO는 전체 이미지를 직접적으로 학습하고 간단하게 구축할 수 있는 모델이다.
- Classifier-based approaches와 달리 detection performance에 알맞은 손실함수를 정의했고 모델에 이를 수용했다.
- Fast YOLO는 가장 빠른 general-purpose objection detector이고, YOLO는 real-time 객체 탐지에서 SOTA를 기록했다.
- 새로운 도메인에 대해서도 빠르면서 일반화가 잘된 detection이 가능하다.

<br><br>

# 개인적인 생각
- 
<br><br>

# 이미지 출처
- [You Only Look Once: Unified, Real-Time Object Detection Paper](https://arxiv.org/pdf/1506.02640v5.pdf)
