---
layout: single
title: "Section1 Sprint3 Note132 Covariance, Correlation, Linear Algebra2"
categories: Section1
toc: true
toc_sticky: true
---

선형대수 두번째 파트로 공분산, 상관계수, 직교, 선형투영, Span, Basis, Rank 등에 대해 배웠다.

### Variance(분산)
분산은 평균으로부터 데이터가 흩어져있는 정도를 나타낸다.

![image](https://user-images.githubusercontent.com/97672187/152782925-59a944e9-4070-4e1c-9aa8-910c9d2b9f18.png)

평균과 관측치의 차이(편차)의 제곱을 관측치의 수 만큼 나누기때문에 편차 제곱의 평균이라고 할 수 있다.
분산이 클수록 데이터가 많이 흩어져있다는 뜻이다.

### Standard Deviation(표준편차)
분산의 제곱근(분산에 루트를 씌운것)으로 분산은 제곱을 하기 때문에 평균보다 scale이 커지니까 이를 낮추기 위해 루트를 씌운다.

### Covariance(공분산)
공분산은 1개의 변수가 변화할 때 다른 변수가 얼만큼의 연관성을 나타내며 변하는지를 측정하는 것이다. 따라서 공분산은 다음과 같이 표시할 수 있다.

**_Cov(X,Y)_** > 0: X가 증가하면 Y도 증가한다. (양의 선형관계)

**_Cov(X,Y)_** < 0: X가 증가하면 Y는 감소한다. (음의 선형관계)

**_Cov(X,Y)_** = 0: X와 Y는 연관이 없다.

- 공분산이 크면 두 변수간의 연관성이 크다고 할 수도 있겠지만, 만약 두 변수의 스케일이 너무 크다면 연관성이 적더라도 스케일 때문에 공분산이 크게 나오기 때문에 공분산은 scale로 부터 받는 영향이 크다.
- 예시는 Discussion 참고.

```python
#공분산 구하는 방법
#1) DF에서 구하기
df.cov()

#2) np 사용
np.cov(A,B)
```

### Correlation coefficient(상관계수)
위에서 공분산의 단점은 scale의 영향을 너무 많이 받는다는 것이었다. 그렇다면 scale의 영향을 어떻게 줄일 수 있을까?

방법은 계산한 공분산을 두 변수의 표준편차로 각각 나누는 것이다. 공분산을 표준편차로 나누면 scale이 -1과 1사이의 값으로 모두 변하게 된다.

이렇게 되면 공분산처럼 절대적인 크기에 영향을 받지 않기 때문에 보다 좋은 지표로 사용할 수 있게 된다. 공분산은 항상 scale, 단위를 포함하고 있지만 상관계수는 이에 영향을 받지 않는다.
또한, 상관계수는 데이터의 평균 혹은 분산의 크기에 영향을 받지 않는다.

**_Cor(X,Y)_** < 1 and > 0 : X가 증가할 때 Y는 증가한다. (양의 상관관계)
 
**_Cor(X,Y)_** > -1 and < 0 : X가 증가할 때 Y는 감소한다. (음의 상관관계)

**_Cor(X,Y)_** = 0 : X와 Y는 상관관계가 없다.

```python
#1) DF로 상관관계 구하기
df.corr()

#2) np사용
np.corrcoef(A,B)
```

파이썬에서는 보통 Numeric변수로 상관관계를 표현하지만, Spearman correlation을 Categorical 변수에도 상관관계를 구할 수 있다.

### Orthogonality(직교)

직교란, 벡터 혹은 매트릭스가 서로 수직으로 있는 상태이다. 위에서 공분산은 한 변수의 변화에 따른 다른 변수의 연관성이라고 했다. 하지만 벡터가 직교하면 두 변수 사이에는 어떠한 연관성도 존재하지 않는다.

### Unit Vectors(단위 벡터)
단위 길이(크기)가 1인 벡터를 말한다.
원래 v = [1,2,2] 일 때,

v_norm = sqrt(1+4+4) = 3 이 된다.

여기서 v를  v_norm으로 나눈다면

v_hat = v / v_norm = [1/3, 2/3, 2/3]

v_hat_norm = sqrt(1/9 + 4/9 + 4/9) = 1

모든 벡터는 위의 식 처럼 단위 벡터로 표현할 수 있다.

### Span
Span은 주어진 벡터를 조합(더하거나 곱함, 선형 결합)하여 만들 수 있는 모든 가능한 벡터의 집합이다. (벡터의 집합으로 표현할 수 있는 차원이라고 생각하면 될 듯. ex) Span = 평면, 선, 3차원 등등)
- 두 벡터가 선형종속관계(or 선형관계)에 있다는 것은 두 벡터가 동일 선상에 있다는 뜻이다. 
- 따라서 원래 벡터가 2개면 2차원 평면의 공간을 표현할 수 있는데 선형종속관계의 두 벡터라면 평면이 아닌 1차원 선으로만 공간을 표현할 수 있다.
- 반대로 두 벡터가 선형독립관계(or 비선형관계)에 있다면 두 벡터는 원래의 벡터의 갯수대로 2차원 평면의 공간을 표현할 수 있다.

### Basis
Basis는 Span과 반대되는 개념으로 공간이 주어졌을 때 이 공간을 표현할 수 있는 비선형관계의 벡터들의 모음을 말한다.
- Orthogonal Basis는 Basis 개념에 추가로 주어진 공간을 채울 수 있는 서로 수직인 벡터의 모음을 말한다.
- Orthonomal basis는 Orthogonal Basis에 추가로 Normalized의 조건이 붙어서, 길이가 서로 1인 벡터들을 말한다.

### Rank
Rank는 매트릭스나 벡터들로 만들 수 있는 (Span) 공간의 차원을 말한다. (최대 몇 차원까지 만들 수 있는지)

```python
#np 사용해서 rank 쉽게 구하기
g = np.array([1,3])
h = np.array([2,6])
g_h_mat = np.stack((g,h)) # 벡터들을 합쳐서 2차원으로 만들어줌.
#위에처럼 벡터를 합쳐서 matrix로 넣어야함.
res6 =  np.linalg.matrix_rank(g_h_mat) # span (차원 수)
```

ex1)

만약 벡터는 3갠데 rank가 2라면 한 벡터는 다른 2개의 벡터와 선형 결합으로 이루어져 있다는 것이다
-> 벡터가 3개일 지라도, 선형관계에 있는 벡터가 존재하기 때문에 span은 2차원으로 밖에 표현을 못한다.

ex2)
만약 벡터도 3개고 rank도 3이라면, 세개의 벡터는 모두 비선형관계 즉, 선형 독립관계이고 span은 3차원으로 표현할 수 있다.

### Linear Projections(선형 투영)
PCA(주성분분석)는 차원을 계속 축소해나가며 변수의 중요도를 파악함으로써 중요도가 높은 변수만 남겨두어 모델링을 할 때 cost를 줄이고, 모델의 성능을 높이는 기법이다.

여기서 사용되는 개념이 Linear Projection과 비슷하다고 할 수 있다.

2차원 공간에서의 투영은 다음과 같다.

w와 v라는 벡터가 있을 때, 두 변수 중 하나만 사용하기 위해서(cost 절감) w변수를 v 변수로 투영시켜보자.

![image](https://user-images.githubusercontent.com/97672187/152787861-247d6dfc-1578-48b4-b677-3054b644f4f9.png)

1. w' 은 v에 스칼라인 c를 곱함으로 표현 할 수 있다. 또 v,w는 모두 크기를 나타내기 때문에(제곱에 루트 씌운 것) 피타고라스 정의에 의해 w' = w-y가 성립한다.
2. y를 좌변으로 넘긴다.
3. 양변에 v를 곱하면 v와 y는 Orthogonality(직교) 하기 때문에 내적을 하면 0이 된다. (핵심. 두 벡터가 직교하면 내적은 0이다.)
4. 따라서 w' = cv이고 다음과 같이 나타낼 수 있다.

![image](https://user-images.githubusercontent.com/97672187/152789398-dca05cc3-7a7e-4cac-a643-7c65ba2f3f76.png)

```python
v = [7, 4]
#v를 xy에 투영시킨다.
#v prime은 xy에 투영된 v 벡터
# projection = xy * (dot(v,xy)) / xy_norm^2
def myProjection(v):
  v = np.array(v)
  x,y = 3,3
  xy = np.array([x,y])
  xy_norm = np.sqrt(sum(xy**2))
  return (xy*(np.dot(v,xy))) / xy_norm ** 2

vprime = myProjection(v)
print(vprime)
```

```python
import matplotlib.pyplot as plt

plt.xlim(0,8)
plt.ylim(0,8)

plt.arrow(0, 0, v[0], v[1], linewidth = 3, head_width = .05, head_length = .05, color = 'blue') # xy로 투영시킬 v
plt.arrow(0, 0, vprime[0], vprime[1], linewidth = 3, head_width = .05, head_length = .05, color = 'green', linestyle = 'dashed') # xy로 투영된 v
plt.arrow(0, 0, xy[0], xy[1], linewidth = 3, head_width = .05, head_length = .05, color = 'red') # xy

plt.title("Shadow of v")
plt.show()
```

### Discussion

1) heatmap을 통해 확인할 수 있는 데이터 간의 관계는 무엇인가요?
heatmap은 각 변수들의 상관관계를 시각화한 그래프이다. 색이 진할수록 상관관계가 높은 것이고, 상관관계가 높다는 것은 한 변수가 변화할 때 다른 변수도 이 변수의 변화에 비례 혹은 반비례 하여 변하는 정도가 크다는 것이다. heatmap은 꼭 박스모양과 색깔로만 정도를 표현할 수 있는 것이 아니며, 사용자가 원하는 대로 옵션을 조정할 수 있다. 얼만큼의 양의 혹은 음의 상관관계를 띄는지 확인할 수 있다.

2) 공분산과 상관계수의 수식을 확인하고 그 둘의 연관성에 대해 설명해주세요.

![image](https://user-images.githubusercontent.com/97672187/152790002-5f77a979-76be-4f88-b393-07a572494bba.png)

이미치 출처: http://micro-met.blogspot.com/2012/05/blog-post.html

공분산은 한 데이터가 변할 때 다른 데이터가 얼마나 민감하게 변하는지를 말한다.

즉, X의 편차와 Y의 편차를 곱한 것의 평균이다. 따라서 공분산이 0보다 크다는 것은 X가 증가할 때 Y도 증가하고, 0보다 작다는 것은 X가 증가하면 Y는 감소한다는 뜻이다. 공분산이 0이라는 것은 X와 Y가 선형독립관계라는 것이고, 별 상관이 없다고 해석할 수 있다.

공분산은 데이터의 상관성을 나타내지만 데이터의 scale에 영향을 매우 많이 받는다. 예를 들어, 키와 몸무게의 상관관계를 파악한다고 했을 때 한 데이터는 키의 단위를 m 단위로(180이면 1.8m), 한 데이터는 키의 단위를 cm단위로(180이면 180cm) 표현했다고 가정하자. 그렇다면 키가 클수록 보통 몸무게가 많이 나갈텐데, 단위를 다르게 표현해도 두 값은 결국 똑같은 데이터이기 때문에 같은 상관성을 가져야한다. 하지만 cm단위로 표시한 데이터가 m 단위로 표시한 데이터보다 공분산이 크게 나온다면 이 관계를 신뢰 할 수 없다. 같은 데이터인데 공분산이 더 크다고 연관성이 큰 것이 아니다.

이러한 이유 때문에 상관계수는 이 scale이라는 것을 -1과 1 사이로 단위화시킴으로써 데이터가 scale의 영향을 크게 받지 않게 한다. 위에서 p의 공식을 보면 상관계수 역시 공분산을 표준편차만큼 나눈 것이기 때문에 공분산과 같이 데이터의 연관성을 확인하는 지표라는 것을 알 수 있다. 공분산과는 값의 범위가 다르기 때문에 상관계수가 1에 가까울수록 양의 상관관계, -1에 가까울수록 음의 상관관계, 0에 가까울수록 상관관계를 갖지 않는다.

3) 상관관계와 인과관계는 무엇이 다른지 설명해주세요.
데이터 과학자에게 상관관계는 꼭 인과관계를 의미하는 것이 아니다. 두 가지 관계 모두 데이터들간의 연관성을 말하는 것 같다. 하지만, 두 가지 변수가 있다고 했을 때 x라는 변수가 y라는 변수를 예측할 때 유용할 수 있어도 x가 y의 원인이라고 확정지을 수 있다는 것은 아니다. y가 x의 원인이 될 수도 있고, 더 나아가면 x,y가 다른 특정 변수 때문에 우연히 연관성을 띌 수도 있다.

예를 들어 축구에서 득점을 많이 한 선수가 발롱도르(한 해동안 세계 최고 선수에게 주는 상)를 많이 수상했다면 득점과 발롱도르 수상횟수는 양의 상관관계를 가질 것이다. 최근 15년간의 데이터를 보면 호날두와 메시가 10년이상을 번갈아가면서 발롱도르를 수상했고, 득점 또한 정말 많이(전 세계에서 거의 가장 많이)했다. 이 데이터만 보면 득점과 발롱도르는 연관성이 크고 득점이 발롱도르의 원인처럼 보인다. 2018년에는 메시는 득점왕을 했고, 리그를 우승시켰고, 호날두 또한 이탈리아 리그에서 맹활약하며 팀을 우승시켰다. 하지만, 시즌동안 4골에 8도움 밖에 기록하지 못한 모드리치가 발롱도르를 수상했다. 챔피언스리그 우승과 월드컵 준우승이라는 커리어 덕분에 득점과 도움이 전혀 좋은 기록이 아닌 것처럼 보이는데도 세계 최고의 선수의 자리에 오를 수 있었다. 이를보면, 득점이 꼭 발롱도르 수상의 원인이라고 즉, 득점과 발롱도르 수상횟수는 상관관계가 있지만, 인과관계가 있다고 보긴힘들다. 각 포지션마다 중요하게 생각하는 지표, 대회 수상 경력 등 다양한 지표들이 서로 영향을 주어 발롱도르 수상이라는 데이터를 만들어낸다. 이 예시에서 인과관계란, 만약 데이터에 골 수와 득점왕 유무라는 데이터가 있다면 이 두 데이터는 인과관계가 있을 것이다. 골 수가 가장 많은 사람이(원인) 당연히 그 리그의 득점왕(수상 경력)이 될 테니까. 그렇다면 이 득점왕이라는 수상 경력이 발롱도르 수상에 영향을 미칠 것이다.(상관관계 형성)

결론: 두 가지 관계 모두 데이터들의 연관성을 나타내지만, 내 생각에는 인과관계는 데이터들 간에 얼마나 연관성이 있느냐의 정도가 아니라 연관성을 True or False 처럼 명확하게 단정지을 수 있는 관계이고, 상관관계는 연관성이 있는 정도(True or False도 되지만, 얼마나 있느냐도 가능)를 파악할 수 있는 관계라는 점에서 다르다고 생각한다. 따라서 상관관계는 인과관계를 포함하는 것 같다.

인과관계 -> 무조건 상관관계라고 할 수 있다. but 상관관계 -> 무조건 인과관계라고 할 수 없다.







