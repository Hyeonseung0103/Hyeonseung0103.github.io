---
layout: single
title: "Note 423 RNN, LSTM, GRU, Attention"
toc: true
toc_sticky: true
category: Section4
---

순환 신경망(Recurrent Neural Network)를 사용하면 시계열과 같은 Sequential한 데이터를 잘 예측할 수 있다. 이번 포스팅에서는 언어모델과 순환신경망에 대해 정리해보자.

### 언어 모델(Language Model)
언어 모델은 문장과 같은 단어 시퀸스에서 각 단어의 확률을 예측하는 모델이다. 지난 포스팅에서 다룬 Word2Vec도 이 언어 모델 중 하나라고 할 수 있는데 예시로 주변 단어들을 기반으로
중심단어가 등장할 확률을 예측하는 CBoW를 들 수 있을 것이다. CBoW는 중심 단어 $ W_{t} $ 를 예측하기 위해 중심단어와 연속적으로 이루어져 있는 $ W_{t-1}, W_{t-2}, W_{t+1}, W_{t+2} $
를 활용하는 언어 모델이다. 

이 언어 모델은 어떠한 단어가 등장했을 때 다른 단어들이 등장할 확률 즉, 조건부 확률의 개념을 사용하여 표현할 수 있다. "He is a good man"이라는 문장이 각 단어들의 조건부 확률로 어떻게 
표현되는지 수식으로 나타낼 수 있다.

예시) "He is a good man"

$ P("He", "is", "a", "good", "man") = P("He") * P("is" | "He") * P("a" | "He is") * P("good" | "He is a") * P("man" | "He is a good") $


### 통계적 언어 모델(Statistical Language Model, SLM)
SLM은 단어의 등장 횟수를 기반으로 조건부 확률을 계산한다.

위의 예시에서 첫번째 항인 $ P("He") $ 를 구해보면, 전체 문장에서 "He"가 들어간 문장을 계산한다. 만약 100개의 문장에서 10개의 문장에 "He"가 들어가 있으면,

$$ P("He") = \frac{10}{100} $$

다음으로, "He"로 시작하는 문장에서 "is"가 바로 뒤에 오는 문장의 빈도를 또 계산한다. 위에서 계산된 10개의 문장에서 8개의 문장에 "is"가 뒤따르면,

$$ P("is" | "He") = \frac{8}{10} $$

위와 같은 방법으로 SLM에서는 해당 문장이 생성될 확률을 각 단어의 확률값으로 계산할 수 있다. 하지만, 빈도로 확률을 계산하기 때문에 한 번도 등장하지 않은 단어에 대해서는 확률이
0이 되므로 적절한 문장을 생성해낼 수 없다는 한계점을 가지고 있다. 이렇게 실제로 사용되는 표현인데 말뭉치에 등장하는 않아서 많은 문장에 등장하지 못하게 되는 문제를 희소(Sparsity) 문제
라고 한다. SLM에서는 이를 해결하기 위해 N-gram이나 smoothing, back-off와 같은 방법이 사용된다고 한다.


### 신경망 언어 모델(Neural Langauge Model)
언어 모델은 통계적 언어 모델 외에도 신경망 언어 모델이란 것이 있다. 신경망 언어 모델은 Word2Vec, FastText와 같이 횟수기반대신 임베딩 벡터를 사용함으로써 말뭉치에 등장하지
않은 단어가 있더라도 의미나 문법적으로 유사한 단어라면 확률을 계산할 수 있게 된다. 비슷한 위치에 있는 단어는 비슷한 의미를 같는다는 분포 가설이 활용되는 것이다.

### 순환 신경망 (RNN, Recurrent Neural Network)
신경망 언어 모델에는 RNN이라는 것이 있다. RNN은 데이터의 순서에 따라 의미가 달라지는 Sequential Data를 잘 처리하기 위해 고안된 신경망이다. 이미지 데이터나 일반적은 데이터 테이블은
데이터들의 index가 바껴도 의미가 크게 달라지지 않는다(Non Sequential Data). 하지만 시계열, 자연어와 같은 Sequential Data는 index가 바뀌면 데이터들의 의미가 크게 달라진다. 문장에서
단어의 순서를 바꾸면 문법적으로 이상한 문장이 될 수 있고, 주식과 같은 시계열 데이터는 전날과 오늘의 데이터를 맘대로 바꿔서 해석하면 올바른 예측이라고 할 수 없다.

- RNN의 구조

![image](https://user-images.githubusercontent.com/97672187/167328520-8604a39a-f40a-4b89-bc38-2275bcd64b1e.png){: .align-center}

이미지출처: https://zetawiki.com/wiki/%EB%B0%94%EB%8B%90%EB%9D%BC_RNN

위에 그림을 보면 RNN은 3개의 화살표로 이루어져있고 $ X_{t} $ 는 입력층, $ h_{t} $ 는 은닉층, $ o_{t} $ 는 출력층(결과값)이다. 가장 아래있는 화살표는 입력 벡터가 은닉층에 들어가는
화살표, 가장 위에 있는 화살표는 은닉층에서 나온 출력 벡터가 output으로 변환되는 화살표, 가운데 있는 화살표는 은닉층에서 나온 벡터를 다시 은닉층의 입력으로 사용하는 것을 나타낸다.
은닉층에서 나온 결과를 다시 은닉층에 전달하는 순환구조이기 때문에 순환 신경망이라는 이름이 붙여졌다. 
이 순환을 timestamp(timestamp는 몇번째 단어인지)별로 풀어서 표현하면 오른쪽 항으로 나타낼 수 있다. 

- RNN 작동 원리

![image](https://user-images.githubusercontent.com/97672187/167336580-cc739394-83c0-4592-9430-196d023b49d7.png){: .align-center}

이미지출처: http://dprogrammer.org/rnn-lstm-gru

1) 이전 은닉층에서 넘어온 출력 벡터와 은닉층의 가중치 행렬을 곱한다.

2) 현재층에서 입력된 벡터를 입력층에서의 가중치 행렬과 곱하고, 이전 은닉층의 결과와 편향을 더한다.

3) 더해서 만들어진 출력벡터를 하이퍼볼릭탄젠트 활성화 함수를 지나 하나는 output으로, 하나는 다음 timestamp으로 전달한다.

! output과 다음 은닉층에 전달된 결과는 같고  $ h_{t} $ 로 표현할 수 있다.

$$ h_{t1}   = tanh(h_{t-1} * W_{h} + x_{t} * W_{x} + b} $$ 

위 과정을 반복하면 데이터의 순서 정보가 기억되기 떄문에 RNN은 Sequential 데이터를 다룰때 많이 사용된다.

- RNN 종류

![image](https://user-images.githubusercontent.com/97672187/167337364-708bf94e-10aa-4d3a-85c9-b3713c9334c4.png)

이미지출처: http://karpathy.github.io/2015/05/21/rnn-effectiveness/

1) one to one: 1개의 벡터를 받아 1개의 Sequential한 벡터를 반환한다. 이미지를 입력받아서 이 이미지를 설명하는 문장을 만들어내는 Image captioning에 사용될 수 있다.

2) many to one: 여러개의 Sequential한 벡터를 입력받아 1개의 벡터를 반환한다. 감성 분석, 스팸 분류 등에 사용될 수 있다.

3) many to many(1): 여러개의 Sequential한 벡터를 입력받아 여러개의 벡터를 반환한다. 여러 문장을 입력받아 번역 문장을 출력하는 기계 번역에 사용된다.

4) many to many(2): 여러개의 Sequential한 벡터를 입력받아 입력받는 즉시 여러 벡터를 반환한다. 비디오를 프레임별로 분류할 때 사용된다. 프레임을 입력받으면 바로바로 어떤 프레임인가를
분류하는 것.

- RNN의 장단점

RNN은 모델이 이론적으로 간단하고, 어떤 길이의 sequential 데이터도 처리할 수 있다는 장점이 있다. 하지만, 벡터가 timestamp순으로 순차적으로 입력되기 때문에 GPU의 장점인
병렬화를 사용할 수 있다. 또한, 역전파 과정에서 활성화 함수인 tanh의 미분값을 사용하게 되는데 tanh의 미분값은 가중치가 -4 ~ 4 밖의 범위에서는 거의 0에 가까운 값을 나타내기 때문에 
층이 깊어질수록 앞에 있는 층에는 손실 정보가 잘 전달되지 않는 기울기 소실(Vanishing Gradient)이 발생한다. 반대로, 미분값이 조금만 커도 층이 깊어질수록 그 미분값이 제곱의 제곱의 제곱이
되어서 앞층에는 손실 정보가 과하게 전달될 수 있는 기울기 폭발(Exploding Gradient)이 발생할 수도 있다.

RNN에서 발생할 수 있는 기울기 소실은 LSTM, GRU를 사용함으로써 해결할 수 있다.

### LSTM(Long Short Term Memory)
LSTM은 RNN에서 발생하는 기울기 소실 문제를 해결하기 위해 역전파 과정에서 전달되는 기울기 정보를 조정하기 위한 Gate를 추가한 모델이다. 보통 RNN을 사용한다고 하면, 단순한 RNN(Vanilla RNN)
이 아니라 LSTM을 의미한다. 

![image](https://user-images.githubusercontent.com/97672187/167348205-68c74d50-734a-4dc6-ac41-4e502fa420c8.png){: .align-center}

LSTM에서는 위의 그림에서 $ f_{t} $ 부분에 해당하는 forget gate, $ i_{t} $ 와 $ g_{t} $ 에 해당하는 input gate, $ o_{t} $ 에 해당하는 output Gate가 존재한다.
forget gate는 이전 time_stamp의 정보를 얼마나 유지할 것인지, input gate에서는 현재 time_stamp에서 새로 입력된 정보를 얼만큼 반영할 것인지, output gate에서는 forget과 input gate
에서 계산된 출력 정보를 다음 은닉층에 얼만큼 넘겨줄지를 정하는 게이트이다. 

LSTM은 RNN과 달리 hdden state 외에도 cell state($ C_{t} $) 라는게 추가 되었다. cell-state는 활성화 함수를 거치지 않아서 정보의 손실이 없기 때문에 이전 시퀸스의 정보를 완전히
잃지 않으면서 뒷쪽 시퀸스에 현재 정보를 얼만큼 전달할 것인지 결정할 수 있다.

- LSTM 작동 원리

![image](https://user-images.githubusercontent.com/97672187/167350211-a39e570f-0120-42d5-8cd0-0d65c71aa4fd.png){: .align-center}

이미지출처: http://dprogrammer.org/rnn-lstm-gru

1) forget gate

이전 은닉층의 출력값과 현재 입력층의 입력값을 forget gate의 가중치 행렬과 곱해서 더한다. 계산된 결과를 시그모이드 함수를 통과시켜서 이 정보를 얼만큼 기억할 것인지 정한다. 시그모이드
값이 0이면 이전 정보를 모두 잊고, 1이면 모두 기억한다. 이 활성화값을 기반으로 이전 cell state의 정보를 얼만큼 기억할지 결정한다.

2) input gate

이전 은닉층의 출력값과 현재 입력층의 입력값을 input gate의 가중치 행렬과 곱해서 더한다. 계산된 결과를 시그모이드 함수를 통과시켜서 input gate에서 발생할 정보를 얼만큼
반영할 것인지 정한다($ i_{t} $). 또한, 시그모이드에 입력한 똑같은 입력벡터를 하이퍼볼릭탄젠트를 취하고($ g_{t} $) 여기서 나온 벡터와 시그모이드에서 나온 벡터를 내적해서 
input gate에서 얼만큼의 정보를 내보낼 것인지 결정한다. 즉, 새로운 정보를 cell state에 얼만큼 저장할지를 결정한다. 시그모이드 값은 1에서 가까울수록 정보를 많이 저장, 하이퍼볼릭탄젠트는
1과 -1의 범위 중 1에 가까우면 최대한 많은 현재 정보를 저장, -1에 가까울수록 최소한의 정보를 저장한다는 의미이다.

3) output gate
따로 gate를 분류하진 않았지만, 이렇게 과거를 위한 $ f_{t} $, 현재 정보를 위한 $ i_{t} $ 와 $ g_{t} $ 가 구해졌다면 이들을 각각의 알맞은 cell state와 원소곱해서 더한후 
cell state를 업데이트 시킨다($ C_{t} $).

그 후 output gate에서는 이전 은닉층의 출력값과 현재 입력층의 입력값을 output gate의 가중치 행렬과 곱해서 더한다. 계산된 결과를 시그모이드 함수를 통과시키고 위에서 업데이트 된 현시점의 cell state에
하이퍼볼릭탄젠트 함수를 통과시켜서 두 결과를 내적하면 출력값과 다음층으로 전달되는 값($ h_{t} $)을 최종적으로 구할 수 있게 된다.
output gate에서 발생한 정보($ o_{t} $)를 얼만큼 다음 hidden state에 전달할지 정한다($ h_{t} $).




LSTM과 GRU는 기울기 소실 문제를 보완하려고 나옴. 얘도 Timestamp별로 해결해서 병렬화는 안 됨.

sigmoid를 계산한 값으로 gate를 얼만큼 열지 결정. 0.2가 나오면 20%만 연다.

output gate는 다음 hidden gate의 값을 정해주는 것이다(forget gate 다음으로 얼마나 남길지).

LSTM은 3개의 gate마다 가중치가 다름. 따라서 연산량이 많다. 이를 간소화시킨게 GRU

처음에는 state의 값이 다 0이다. 그 다음에 새로 입력된 정보를 추가하면서 업데이트가 된다.

용량을 100으로 보고 forget gate와 inpute gate 중 하나만 알면 다른 gate의 정보를 얼만큼 반영할 것인지 정할 수 있는 


NLP에서는 데이터를 (batch_size, sequence length, embedding dim)으로 넣는다. (데이터 갯수, 문장의 길이, 임베딩 벡터 차원)






