---
layout: single
title: "Mask R-CNN 논문 요약"
toc: true
toc_sticky: true
category: Segmentation
---

# Abstract
본 논문에서는 object instance segmentation task에 대한 simple, flexible, general한 framework에 대해 소개한다. 본 연구의 접근방식은 효과적으로 객체를 탐지하는 동시에 각각의 인스턴스에 대해 높은 수준의 segmentation mask를 생성한다. Mask R-CNN은 Faster R-CNN에 layer를 확장시켜 object mask를 병렬로 예측하고 기존의 box recognition도 그대로 수행할 수 있다. Mask R-CNN은 학습이 쉽고 Faster R-CNN보다 연산이 크게 추가되지 않은 5 fps의 속도로 동작한다. 나아가, 사람의 동작을 예측하는 task 등 다양한 task를 수행할 수 있다. COCO datset에서 instance segmentation, bounding box object detection, person keypoint detection에 대해 좋은 성적을 거두었다. COCO 2016의 우승팀을 포함하여 기존에 존재한 모든 single model보다 우수한 성적을 거두었다. 본 연구가
향후 instance-level recognition 분야에 큰 도움이 되길 바란다.

<br><br>

# Details
## Introduction
- Object detection과 semantic segmentation은 Fast/Faster R-CNN과 FCN처럼 좋은 baseline을 사용해 짧은 기간동안 많은 발전을 이뤘다.
- 본 연구의 목표는 다른 task들과 비슷한 수준으로 instance segmentation이 가능한 framework를 만드는 것이다.
- Instance segmentation은 모든 객체에 대해 정확한 detection과 segmentation이 필요하기때문에 어렵다. 따라서, 복잡한 방법이 좋은 결과를 불러올 것이라고 생각했는데 굉장히 단순하고 유연하고 빠른 시스템이 instance segmentation의 SOTA를 넘었다.
- Mask R-CNN은 Faster R-CNN을 확장시켜 각 RoI에 대해 segmentation mask를 예측하고, 병렬적으로 기존과 같이 classification, bounding box regression을 수행한다. Mask branch는 작은 FCN이 RoI에 적용된 것이고 pixel 단위로 segmentation mask를 예측한다. 또한, mask branch는 작은 계산만 추가되기때문에 기존처럼 빠른 시스템을 만들 수 있었다.
- Faster R-CNN은 pixel-to-pixel로 설계되지 않았기때문에 **RoIPool**이 특징 추출을 위해 공간적인 정보를 coarse(거칠게. pooling을 수행하여 일부 공간정보가 손실되기때문에 이렇게 어느 정도 희생을 감소해서 특징을 추출하는 것을 coarse하다고 함)하게 추출한다. 이러한 misalignment 문제를 해결하기 위해 본 연구에서는 간단하고 공간적인 제약에서 자유로운 **RoIAlign** 기법을 사용해서 공간 위치를 잘 보존한다. 이 간단한 변화가 mask 정확도를 10%에서 50%까지 올렸다.
- 또한, mask와 class prediction을 분리하는 것이 매우 중요하다는 것을 알았다. 각각의 class끼리 영향을 주지않도록 class에 대해 binary mask를 독립적으로 수행했고, class를 예측하기위해서는 RoI classification에만 의존했다.
- 반면, FCN은 보통 픽셀단위에서 multi-class categorization을 수행하는데 이는 segmentation과 classification을 묶은 것이고, 이 방법은 경험상 instance segmentation 분야에서 좋지 않은 방법이다.
- Mask R-CNN은 종과 호루라기 말고는 COCO instance segmentation task에서 이전 대회의 SOTA를 뛰어넘었다. GPU에서 프레임당 200ms가 소요됐고 COCO dataset을 학습했을 때 하루에서 이틀 정도의 시간이 걸렸다. 이렇게 학습과 test가 빠르고 정확도도 뛰어나기때문에 향후 연구에 잘 쓰일 모델이라고 기대한다.
- Human pose estimation에서도 좋은 속도와 성능을 기록했다.

<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/7db88c37-8c1c-4f7a-85a0-371ff7049cc1">
  </p>
</div>

<br>

<br><br>

## Related Work
**R-CNN**
- R-CNN은 빠르고 정확한 모델로 개선시키기위해 RoIPool을 사용하여 피처맵에서 미리 정의된 RoI에 대한 정보를 추출할 수 있도록했다.
- Faster R-CNN은 RPN을 사용해서 이 기술을 더 발전시켰고 다양한 방법을 동원해서 모델을 유연하고 강건하게 개선시켰다. 그리고 현재, object detection 분야를 이끌어나가는 모델이 되었다.

**Instance Segmentation**
- R-CNN의 영향으로 instance segmentation에 대한 많은 접근들이 segment proposals을 기반으로 이루어져있다.
- DeepMask와 이후의 연구들에서는 segment 후보를 제안하도록 학습한 후 Fast R-CNN을 통해 분류를 수행한다. 이 방법은 segmentation precedes(선행, 우선) recognition으로 느리고 정확도가 낮다는 단점이 있다.
- 본 연구에서는 mask와 class label에 대해 병렬로 예측을 수행해서 간단하고 유연한 모델을 만들었다.
- 가장 최근 Li의 연구에서는 segment proposal system과 object detection system을 결합한 fully convolutional instance segmentation(FCIS)를 개발했다. 이 모델은 position sensitive output channels 집합을 fully convolutional하게 예측하는 것으로 이 채널이 object classes, boxes, masks를 동시에 처리해서 모델을 빠르게 한다.
- 하지만, FCIS는 overlapping instance에 의한 에러와 가짜 edges를 만들어 instance segmentation을 수행하기에는 어려움을 겪었다.
- 다른 접근으로는, 좋은 semantic segmentation을 기반으로 instance segmentation을 수행하는 것이다. FCN outputs과 같이 pixel 단위로 classification을 수행하고 이 결과를 가지고 같은 카테고리에 있더라도 다른 instance로 잘라내려고 시도한다.
- Mask R-CNN은 이러한 segmentation first 기법과 다르게 instance first 기법을 사용한다. 그리고 앞으로 이 두가지 기법을 더 깊게 통합하는 연구가 향후에 있을 것으로 예상한다.

<br><br>

## Mask R-CNN
Mask R-CNN은 개념적으로 간단하다. Faster R-CNN은 한 객체에 대해 class label과 bounding box offsets 이 2가지 outputs을 내는데 Mask R-CNN은 여기에 branch를 하나 더 추가해서 object mask까지 outputs으로 내는 것이다. 그럼 지금부터는 Fast/Faster R-CNN이 놓쳤고, pixel-to-pixel alignment를 포함한 Mask R-CNN의 핵심 요소에 대해 알아보자.

<br>

**Faster R-CNN & Mask R-CNN**
- Faster R-CNN은 1 stage는 RPN, 2 stage는 RoIPool을 통해 특징을 추출하고 분류 및 box regression을 수행하는 Fast R-CNN으로 이루어져있다. Mask R-CNN도 2 stage 모델로 1 stage는 RPN, 2 stage는 class/box offsets/binary mask tasks를 각 RoI에 대해 병렬적으로 수행한다. 이것은 classification에 의존한 mask predictions을 사용하는 최근 모델들과는 다르다는 것을 보여준다.
- 학습 과정에서는 RoI에 대해 $L = L_{cls} + L_{box} + L_{mask}$로 multi-task loss를 정의한다. Classification loss와 box loss는 Fast R-CNN과 같고 mask branch는 mask resolution이 m x m, 총 class 수가 $K$라고 했을 때 각 RoI에 대해 $Km^2$의 사이즈를 갖는 output을 도출한다. 이를 적용하기 위해 pixel 단위로 sigmoid를 사용했고 $L_{mask}$를 average binary cross-entropy loss로 정의했다. RoI내에서 mask들끼리는 서로 연관되어있지않고 독립적인 loss로 사용된다.
- 이렇게 Mask R-CNN은 masked class에 대해서 전용 분류기를 따로 사용해서 mask와 classification 예측을 분리하는 구조다(decouples). FCN에서는 semantic segmentation을 수행할 때 픽셀단위로 softmax와 multinomial cross-entropy loss를 사용해서 픽셀마다 mask들이 다른 class들과 연관되어 mask, classification 예측이 함께 이루어지지만, Mask R-CNN에서는 픽셀 단위로 sigmoid와 binary loss를 사용하기때문에 두 가지 예측에서 class들끼리 엮여지않고 각 예측이 독립적으로 수행되어 연산량의 측면에서 훨씬 효율적이다.

<br>

**Mask Representation**
- Mask는 class labels과 box offsets과 달리 fc층에 의해 어쩔 수 없이 공간정보가 손실된 짧은 벡터로 변환되는데 mask에 대한 공간적인 정보를 추출하는 것은 CNN에서 pixel-to-pixel로 작업이 이루어지기때문에 크게 걱정할 필요없다. 특히, 각 RoI에 대해 mxm으로 mask를 예측할 때 FCN(Fully Convolutional Layer)을 사용하기때문에 공간정보의 손실이 존재하는 벡터가 아닌 상태로 mxm의 공간정보를 잘 유지시킬 수 있다.
- 이런 pixel-to-pixel 방법을 활용하기 위해서는 그 자체가 작은 피처맵이라고 할 수 있는 RoI의 픽셀당 공간 정보가 잘 정렬이 되어있어야하는데 본 연구팀의 RoIAlign 기법이 mask prediction을 위한 핵심 역할을 수행한다.

<br>

**RoIAlign**
- RoIPool은 다양한 크기의 RoI를 CNN에 집어넣기위해 고정된 피처맵(e.g. 7x7)으로 변환하는 역할을 수행했다. 먼저, 소수 형태로 이루어져있는 RoI를 이산화(정수형태로 변환. e.g. [x/16] x를 16으로 나누고 정수부분만 사용)하고 해당 RoI를 고정된 크기의 피처맵으로 출력하기 위해 주로 max pooling을 사용하여 aggregate 시킨다. 하지만, 이렇게 소수 형태로 이루어져있는 RoI와 quantization(연속적인 좌표값을 피처 맵의 그리드에 딱 맞게 떨어지는 값으로 변환하는 과정)이 수행된 피처맵이 misalignment되는 경우가 많기때문에 classification에는 영향을 미치지않더라도 pixel 단위로 이루어지는 segmentation과 같은 task에는 안 좋은 영향을 끼친다.
- RoIAlign은 이러한 문제를 해결하기 위해 RoIPool에서 harsh quantization된 피처를 재조정해서(실제로 재조정하는게 아니고 개념적으로 보면 더 정확한 align이 이루어짐) input과 피처맵을 정렬시킨다. 방법은 이산화를 시켰던 방법을 소수부분까지 그대로 사용해서 quantization을 막고(e.g. [x/16] -> x/16)bilinear interpolation을 통해 각 RoI의 픽셀들을 가중합시켜서 기존 RoI와 유사하도록 피처맵을 구성한다.
- Figure 3.은 점선이 피처맵, 실선이 RoI(2x2 bin을 가진)라고 할 때, 더 정확한 RoI 피처맵을 만들기위해 각각의 bin을 4개의 sampling points를 사용해서 또 4등분 하여 subsample을 만들고 이 subsample에 대해 bilinear interpoliation을 적용하고 최종적으로 max pooling이나 average pooling을 적용해 특징을 추출하는 것을 설명하는 그림이다.

<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/cb14b555-387b-4b9a-b1b0-7f15424ccb43">
  </p>
</div>

<br>

- 아래 그림은 subsampling에서 bilinear interpolation을 사용하고 최종적으로 max pooling을 적용하는 예시이다. Bilinear interpoliation을 보면 subsample에서 다른 pixel들이 포함된 만큼만 공평하게 비율을 가져가는 즉, 적절한 가중합이 이루어진 것을 알 수 있다.

<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/34b47475-3713-4187-a760-5c8f715d5c1e">
  </p>
</div>

<br>

<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/cc3f305a-3e7f-4b7f-b35a-58379229a7c0">
  </p>
</div>

<br>

- RoIAlign을 통해 큰 성능 향상을 이루어냈다.

<br>

**Network Architetecture**
- Mask R-CNN의 일반화 능력을 입증하기위해 다양한 architecture를 조합하여 사용했다.
  - 전체 이미지에 대해 특징을 추출한 convolutional backbone architecture
  - network head 부분에서는 bounding box recognition(classification & regression)
  - 각 RoI에 대해 mask prediction
- Backbone
  - ResNet과 ResNeXt의 50 혹은 101 layers를 backbone으로 사용했는데 만약 ResNet-50의 final convolutional layer에서 C4라고 불리는 4번째 stage까지의 feature를 사용했으면 ResNet-50-C4라고 칭한다.
  - ResNet-FPN(Feature Pyramid Network) backbone을 사용했을 때 정확도와 속도면에서 우수했다.
- Network head
  - Head에는 Faster R-CNN에 mask prediont branch가 있는 FCN을 추가한게 전부다.
  - 구조는 Figure 4.를 참고하자.

<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/43f9c045-801b-453e-b50d-b0e06c83f4fc">
  </p>
</div>

<br>

### 1. Implementation Details
**Training**
- Fast/Faster R-CNN과 동일한 하이퍼파라미터를 사용했고 이는 object detection을 위해서 사용된 것이지만 instance segmentation에도 잘 맞았다.
- Fast R-CNN에서 IoU thresholds를 0.5로 사용했다. Mask R-CNN에서 mask loss $L_{mask}$는 positive RoI에 대해서만 정의된다.
- 전처리에는 image resizing, mini-batch 2 images per GPU, N RoIs per image, 1:3 positive:negative ratio가 적용됐다. N은 C4 backbone에서는 64, FPN에서는 512로 적용했다.
- 8 GPUs(mini batch 2 * 8 = 16), 160k iterations, lr 0.02(120k iteration에 10배 감소), weight decay 0.0001, momentum 0.09.
- RPN anchors box는 5 scales과 3 aspect ratios를 가졌고 RPN은 명시하지않는 한 Mask R-CNN과 별개로 학습되어 features를 공유하지 않지만, 본 연구에서는 두 네트워크가 backbone이 같기때문에 공유할 수 있다.

<br>

**Inference**
- Test에는 RoI가 C4에서는 300, FPN에서는 1000이 사용된다. NMS가 적용된 Mask branch는 가장 성능이 좋은 100 detection boxes를 예측한다. 이 방법은 학습과 다른 parallel computation이지만 더 갯수가 적고 정확한 RoI를 사용한 것이기때문에 inference 속도를 높이고 정확도를 증가시킬 수 있다.
- Mask branch는 RoI당 K(class 갯수)개의 masks를 예측할 수 있지만 모든 class에서의 mask가 아니라 classification branch에서 예측한 k라는 class에 대해서만 mask 예측을 수행한다(학습에서는 모전체 class에 대한 mask였지만 inference에서는 굳이 전체 class에 대해서 할 필요없음).
- m x m 사이즈로 소수로 이루어진 mask output은 RoI와 같은 크기로 resizing되고 threshold 0.5를 기준으로 binarized 된다.
- Top 100 detection box로만 mask를 예측했기때문에 Faster R-CNN에서 비해 overhead가 조금밖에 증가하지않았다.

<br><br>

## Experiments: Instance Segmentation
### 1. Main Results
<br> 
<div align="center">
  <p>
  <img width="800" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/bf920c64-226b-43e7-b545-c950e21a5762">
  </p>
</div>

<br>

- Mask R-CNN과 SOTA 모델을 비교했고, COCO datasets에서 다양한 AP scale을 가지고 실험을 진행했다. AP는 mask IoU로 평가했다.
- Table 1.을 보면 Mask R-CNN은 모든 AP scale에서 이전 SOTA 모델들을 뛰어넘었다. 종과 호루라기 객체말고는 RestNet-101-FPN backbone모델이 multi-scale train/test, horizontal flip test, online hard example mining등 다양한 기법이 적용된 FCIS+++을 뛰어넘었다.
- Figure 6.는 Mask R-CNN과 FCIS+++을 비교한 것으로 FCIS+++은 겹치는 인스턴스에 대해 artifacts(가짜 인스턴스)를 만들어내는 반면 Mask R-CNN은 인스턴스가 겹쳐도 좋은 예측을 수행하고 있다.

<br> 
<div align="center">
  <p>
  <img width="1000" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/2ca46f6a-bb37-487f-bfae-48d5dd174ab2">
  </p>
</div>

<br>

### 2. Ablation Experiments
<br> 
<div align="center">
  <p>
  <img width="800" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/823421e9-5175-4340-a540-ca62c02da037">
  </p>
</div>

<br>


**Architecture**
- Mask R-CNN은 다양한 backbones을 사용했고 깊은 네트워크(50 vs 101)와 FPN, ResNeXt과 같은 advanced 모델들이 성능 향상에 도움이 됐다(Table 2a). 하지만, 모든 프레임워크가 자동으로 깊고 advanved한 네트워크에서 이점을 누릴 수 있는 것은 아니다.

<br>

**Multinomial vs Independent Masks**
- Table 2b는 FCN처럼 pixel 단위로 softmax와 multinomial loss를 사용하느냐 Mask R-CNN처럼 sigmoid와 binary loss를 사용하느냐의 결과를 비교한 것이다. 쉽게 말해서, classificaiton과 mask가 couple이나 decouple이냐의 비교다.
- Couples tasks를 사용했을때 AP가 5.5 points 하락했다. 이를 통해 다른 class와 상관없이 binary mask를 사용했을때 모델이 더 잘 학습한다는 것을 알 수 있다.

<br>

**Class-Specific vs Class-Agnostic Masks**
- 본 연구에서는 기본적으로 class-specific masks 즉, 클래스마다 mxm의 mask를 만들어냈다. 흥미롭게도 Mask R-CNN에 클래스와 상관없이 하나의 mxm mask만을 생성하는 class-agnostic masks를 적용했더니 생각보다는 효과적이었다. Class annostic masks는 29.7 AP, specific masks(ResNet-50-C4)는 30.3 masks로 큰 차이가 없었다.
- 하지만 기본적으로 사용한 class-specific masks가 성능이 더 좋기때문에 mask와 classification간의 decouple이 중요하다는 것을 보여준다.

<br>

**RoIAlign**
- RoIAlign의 효과를 검증할 때는 ResNet-50-C4 backbone(stride 16)을 사용했고 RoIAlign은 RoIPool보다 3 points 높은 AP_75를 기록했다(Tabel 2c).
- MNC의 RoIWarp(quantization이 되는 것을 허용하지만, bilinear interpolation은 적용하는 기법)와도 비교해본 결과 RoIWarp는 RoIPool보다는 좋지만 RoIAlign보다는 낮은 AP를 기록한다.
- 추가적으로 ResNet-50-C5 backbone(stride 32)을 설계해 RoIAlign을 비교해보았는데 결과적으로 stride가 더 큰 C5 backbone 모델의 AP가 C4보다 높았다(Tabel 2c, 2d). 아무래도 stride가 커져서 RoIPool의 misalignment가 더 심해졌을것이고 이에 따라 RoIAlign의 효과는 커졌을 것으로 판단된다. RoIAlign은 stride가 클수록 detection과 segmentation을 더 잘 수행할 수 있게한다는 것을 알 수 있다.
- FPN을 backbone으로 사용했을 때도 RoIAlign의 성능이 더 좋았다(Table 6.).

<br> 
<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/8d2ebef7-f077-402e-ae22-0e9435446e89">
  </p>
</div>

<br>

**Mask Branch**
- Segmentation은 FCN을 이용한 pixel-to-pixel task를 수행한다. FCN의 효과를 확인하기위해 FCN과 MLP(multi layer perceptions)를 비교했다(ResNet-59-FPN backbone 사용, Table 2e).
- FCN을 사용했을 때 MLP보다 AP가 더 높았다. ResNet-50-FPN backbone을 사용하면 head부분은 FCN으로 이루어져있기때문에 MLP와의 공정한 비교를 위해 FCN head부분은 pretrain 시키지 않았다.

<br>

### 3. Bounding Box Detection Results
- 객체 탐지 성능도 확인하기 위해 COCO datasets에서 SOTA 모델들과 비교했다(mask output을 제외하고 classification과 box outputs만 사용). ResNet-101-FPN backbone을 사용하면 이전 여러 SOTA 모델들보다 성능이 더 좋았다.
- 다른 비교를 진행하기 위해 Mask R-CNN에서 mask branch를 제거한 **Faster R-CNN, RoIAlign** 모델을 만들었다. 이 모델은 RoIAlign을 통해 FPN보다 성능이 더 좋았고 box AP는 Mask R-CNN보다 성능이 좋지 않았다. 이를 통해 Mask R-CNN이 multi-task(detection, clssification, segementation)에 효과적이라는 것을 알 수 있다.
- Mask와 box AP의 성능차이가 크지않는 것을 보아 Mask R-CNN은 object detection과 더 어려운 instance segmentation task의 갭을 크게 줄였다고 할 수 있다(Table 1, 3 비교).

<br> 
<div align="center">
  <p>
  <img width="800" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/6a6ef5b0-52fa-4127-b0c0-7adeb18cd094">
  </p>
</div>

<br>

### 4. Timing
**Inference & Training**
- RPN과 Mask R-CNN stages가 features share하도록 ResNet-101-FPN 모델을 사용했을 때 이미지당 195ms의 처리시간이 소요됐다. ResNet-101-C4는 무겁기때문에 더 오랜 시간이 소요돼서 추천하지 않는다.
- Mask R-CNN이 빠르긴하지만 속도를 최적화시킨 것이 아니기때문에 다양한 이미지 사이즈, proposal number 등을 사용해서 향후 연구에서 정확도와 속도를 더 향상시킬 것으로 기대한다.
- Mask R-CNN은 학습도 빠르다. ResNet-50-FPN에서 8 GPU, COCO trainval_35k를 사용했을 때 32시간, RestNet-101-FPN은 44시간이 걸렸다. 

<br><br>

## Mask R-CNN for Human Pose Estimation
Mask R-CNN으로 K개의 key point types(e.g., left shoulder, right elbox..)을 masking하도록 했다. 이 task가 Mask R-CNN이 flexible하다는 것을 보여준다. 본 연구팀은 human pose 도메인에 대한 지식이 많지 않기때문에 이 task는 Mask R-CNN의 generality를 검증하는 용도로 사용한다. Mask R-CNN에 도메인 지식까지 겸비되면 더 좋은 모델이 만들어질 것이다.

<br>

**Implementation Deatils**
- 새로운 tasks를 수행하기위해 한 instance의 K개의 key points에 대해 training target을 mxm mask 중 하나의 pixel만 foreground(key points)로 라벨링된 one-hot mxm binary mask을 예측하는 것으로 만들었다. Instance segmentation처럼 K keypoints로 서로 독립적으로 예측된다.
- ResNet-FPN을 조금 변형시켜서 사용했고 keypoint-level localization에서는 mask task에 비해 상대적으로 high resolution output이 필요하다는 것을 알았다.

<br>

**Main Resluts and Ablations**

<br> 
<div align="center">
  <p>
  <img width="800" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/8c49de3b-fb75-4e38-9199-afc236f5e8d5">
  </p>
</div>

<br>

- ResNet-50-FPN, Key point에 대한 AP를 평가지표로 사용했을 때 2016 COCO keypoint detection winner보다 높은 성능을 기록했다. Mask R-CNN이 더 간단하고 빠르다.
- 더 중요한 것은 predict boxes, segments, keypoints을 동시에 수행하는 unified 모델인데도 5 fps를 기록했다.
- Key point detection task는 어쩌면 mask보다 더 정확한 localization이 필요하기때문에 RoIAlign의 중요성은 역시나 RoIPool보다 컸다. 

<br><br>

# 개인적인 생각
- 발전이 더딘 instance segmentation분야에서 큰 개선을 이뤄냈고 6년이 지난 지금도 instance segmentation 분야를 대표하는 모델이라는 점에서 굉장히 의미있는 연구인 것 같다.
- RoIPool이 가지고 있었던 misalignment 문제를 RoIAlign이라는 간단한 기법으로 해결했다는 점이 인상깊었다.
- Human pose estimation이라는 기존과는 전혀 다른, 심지어 스스로도 지식이 부족하다고 언급한 도메인에 도전해서 일반화 성능을 검증하려고 한 연구팀의 열정을 느낄 수 있었다.
- Classification과 mask prediction을 분리해서 좋은 성과를 냈지만 decouple의 이유로 class 갯수만큼 mask를 만들어야하고, binary가 softmax보다는 연산이 간단하겠지만 결국 불필요하게 모든 class에 대해 mask를 생성해야한다는 점에서 의문이 들었다. 향후 연구에서는 어떤 방법으로 mask tasks를 좀 더 간단하게 만들지 궁금하다.
- Mask R-CNN도 결국 RPN과 분류기로 이루어진 2 stage 모델이기때문에 1 stage 모델보다 속도면에 있어서는 불리하다. 논문의 저자가 언급한 것처럼 속도에 최적화시킨 모델이 아니기떄문에 Mask R-CNN을 기반으로 향후 연구에서 어떻게 빠르고 정확한 instance segmentation model을 만들지 기대된다.

<br><br>

# 이미지 출처
- [Mask R-CNN Paper](https://arxiv.org/pdf/1703.06870.pdf)
- [RoI Align Image](https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-mask-r-cnn-2018-mask-r-cnn-%EC%8B%A4%EC%8A%B5-w-pytorch-cd525ea9e157)
