---
layout: single
title: "Note 412 Training Neural Network, Backward propagation, Gradient Descent, Loss Function, Optimizer"
toc: true
toc_sticky: true
use_math: true
category: Section4
---

신경망에서의 학습이 어떻게 어떻게 이루어지는지 알아보자. 또한, 손실을 최소화하기 위해서 가중치와 편향을 어떻게 업데이트 시키는지 정리해보자.

### 신경망 학습 과정
1. 데이터가 입력되면 각 층에서 가중치-편향 및 활성화 함수 연산을 수행한다. (순전파)

2. 모든 층에서 연산을 수행한 후 출력층에서 계산된 값을 출력한다. (순전파)

3. 손실 함수를 사용하여 예측값과 실제값의 차이를 계산한다. (손실 계산)

4. 경사하강법과 역전파를 통해 각 층의 가중치를 갱신한다. (역전파)

5. 학습을 중지할 기준을 만족할 때까지 위 과정을 반복한다.

1 ~ 4번까지의 과정을 Iteration이라고 하며 Iteration마다 가중치가 갱신된다.


- 순전파(Forward Propagation)

순전파는 입력층에서 입력된 신호를 은닉층에서 연산을 거쳐 출력층에서 출력값을 내보내는 과정이다. 

- 손실 함수(Loss Function)

손실함수는 손실을 계산하는 함수로 모든 연산을 거쳐 출력층에서 내보낸 값(예측값)과 실제값의 차이를 계산하는 함수이다. 신경망 학습은 이 손실 함수를 최소화하는 방식으로
가중치를 갱신한다.
회귀, 분류에 따라 사용하는 함수가 다르고 같은 회귀 or 분류 문제라도 다른 손실함수를 사용할 수 있고, 회귀에서는 MSE, 분류에서는 Cross Entropy를 주로 사용한다. 

- 역전파(Backward Propagation)

역전파는 순전파와 반대 방향으로 손실 정보를 전달해주는 과정이다. 출력층에서 계산한 손실 정보를 입력층까지 역으로 전달하며 가중치를 갱신하는 알고리즘이다.

- 경사 하강법(Gradient Descent)

역전파의 목적은 손실을 최소화하는 방향으로 가중치를 갱신하는 것인데, 그렇다면 어떻 방법을 사용해서 손실을 최소화 할까?
여기서 사용하는 것이 경사 하강법이다. 손실 함수를 그래프로 나타내면 이 손실 함수의 경사(기울기)가 가장 작아지는 방향으로 업데이트 하여 손실 함수의 값을
줄일 수 있다. 경사가 가장 낮은 부분이 손실이 가장 작은 부분이기 때문에 매 Iteration마다 해당 가중치에서 손실 함수의 미분값을 계산하여 경사가 작아지도록 가중치를 변경한다.

!기울기가 큰 것은 경사가 가파르다는 뜻이기도 하지만, x의 위치가 최소/최대 값에 해당되는 x좌표로부터 멀어져 있다는 뜻이 됨.

<br>


<br>

![image](https://user-images.githubusercontent.com/97672187/165658144-e17bcc3d-035a-46bc-bcbd-7a52b262142c.png){: .align-center}

<br>

<br>

위의 그래프를 보면 기울기가 음수라면 양의 방향으로 x를 이동시켜야 손실을 최소화 시키고, 기울기가 양수면 음의 방향으로 이동시켜야 한다. 이를 수식으로 표현하면 다음과 같다.

![image](https://user-images.githubusercontent.com/97672187/165658427-c8249943-a210-46b3-a1de-361b93fa1e8a.png){: .align-center}

여기서 $x$_i


경사 하강법의 수식을 살펴보자.



















