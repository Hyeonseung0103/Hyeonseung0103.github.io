---
layout: single
title: "VGG(VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION)논문 요약"
toc: true
toc_sticky: true
category: CNN
---

# Abstract
대규모 이미지 인식에서 Convolutional Network의 깊이와 정확도에 대해 연구했다. 상대적으로 작은 3x3 합성곱 필터를 사용하여 층을 깊게 만들었고, 16 - 19개의 층을 쌓았을 때의 모델이 기존에 연구되었던 
모델들의 성능보다 더 좋은 성능을 기록했다. ImageNet Challenge 2014에서 localisation 부문 1등, classification 부문 2등을 차지했다. 본 연구에서 사용된 모델은 다른 데이터셋에서도 좋은 일반화 능력을 보였고, CV 분야에
도움이 되고자 가장 좋은 ConvNet 모델을 공개한다.



<br><br>

# Details

## 도입부
- ConvNet이 활발하게 사용되면서 AlexNet의 논문과 같이 성능을 높이려는 여러가지 시도들이 존재
- 해당 논문에서는 네트워크의 깊이를 늘리는 것에 초점을 맞췄고, 깊이를 늘리는 대신 작은 3 x 3 convolution filter를 사용
- 결과적으로 기존 모델들보다 성능이 좋은 모델을 얻었고, 본 논문에서는 두 가지 모델 소개

<br><br>

## 모델 구조와 분류
### 1. Architecture
- 합성곱층의 깊이에 따라 성능이 달라지는 것을 확인하기 위해 기본적인 구조는 Ciresan의 2011년 논문과 Krizhevsky의 2012년 논문(AlexNet)을 참고
- 224 x 224 크기의 RGB 이미지를 입력받고, 원본 픽셀값에서 평균을 뺀 전처리 이외의 전처리는 수행하지 않음
- 3 x 3 필터를 사용하고 1 x 1 필터를 사용하기도 함
- 합성곱층의 stride는 1로 고정, 특정 합성곱층 뒤에는 max pooling(2x2, stride 2)층 추가
- Fully Connected 계층에서는 첫번째와 두번째 계층에서 4096개의 채널, 마지막 층에서는 1000개로 분류 진행
- 모든 은닉계층에서 ReLU 사용
- Local Response Normalization은 하나의 네트워크에서만 사용. 오히려 연산량과 메모리 소비량을 증가시키는 기법

<br><br>
**3 x 3 filter**

저자의 경우 이전 모델들이 11x11 혹은 7x7 필터를 사용한 것에 비해 3x3이라는 작은 필터를 사용했다. 3x3을 두겹으로 사용하면 5x5의 효과를, 세겹으로 사용하면 7x7의 효과를 낼 수 있다. 예를 들어 7x7이미지가 있을 때 5x5 필터를
사용하면, 

<br><br>

$$\frac{7-5 + 2 \times 0}{1} + 1 = 3$$

<br><br>

3x3의 결과를 얻을 수 있고 3x3 필터를 사용하면,

<br><br>

$$\frac{7-3 + 2 \times 0}{1} + 1 = 5$$

<br>

$$\frac{5-3 + 2 \times 0}{1} + 1 = 3$$

<br><br>
5x5 -> 3x3의 결과를 얻을 수 있다.

그렇다면, 결과적으로 같은 receptive field 효과를 가진다면 굳이 5x5 필터를 한번 사용하지 않고 번거롭게 3x3 필터를 두번 사용하는 이유는 무엇일까? 예를 들어, 채널의 수를 C, 3x3 필터를 2번 사용한다고 가정해보자.
그렇다면 이 층에서의 파라미터 수는 3x3x2xC^2이 될 것이고 만약, 5x5 필터를 한번 사용한다면 이 층에서의 파라미터 수는 5x5xC^2이 될 것이다(C가 제곱인 이유는 input과 output의 채널수를 동일하다고 가정했기때문. 편향은 편의상 제외). 결과적으로, 같은 receptive field 효과를 가지더라도 3x3 필터를 여러번 사용했을때
연산량이 훨씬 줄어들기때문에 3x3 필터를 여러겹 사용하는 것이 더 효과적인 방법이 된다. 또한, ReLU 함수를 여러층에서 통과하게됨으로써 비선형성이 증가해 모델의 학습에 더 도움을 준다.

**1 x 1 filter**
1x1 필터를 굳이 사용하는 이유는 입력과 출력의 채널 수가 같아서 receptive field에 영향을 주지 않는 선형 결합을 할 수 있으면서도 ReLU함수를 통과시켜 출력값에 비선형성을 추가할 수 있다는 장점을 가지고 있기 때문이다.

<br><br>

### 2. Configuration

<div align="center">
  <p>
  <img width="412" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/63090975-11b0-42b4-9239-2ea2d97c011c">
  <br>  
    
  출처 [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION PAPER](https://arxiv.org/pdf/1409.1556v6.pdf)
  </p>
</div>

<br><br>

위의 표는 합성곱층의 깊이를 달리해서 시도한 여러개의 네트워크들이다. A 네트워크는 8개의 합성곱층과 3개의 FC층, E는 16개의 합성곱층과 3개의 FC층으로 이루어져있다. 합성곱층의 깊이만 달리하고 각 층에서의 채널수는 모두 동일하다.
합성곱층의 채널의 갯수는 max pooling이후 2의 제곱으로 증가한다. 각 네트워크마다 이전 네트워크에 비해 추가된 부분은 볼드로 표시했다. 예를 들어 A의 첫번째 합성곱층에는 LRN을 수행하지 않았는데 A-LRN에서는 LRN을 수행했기때문에
볼드로 표시했다.

또한, 아래 표는 각 네트워크별로 사용된 파라미터의 갯수를 나타낸 것이고 3x3 필터를 사용했기때문에 깊이가 깊어진 것에 비해서 사용된 파라미터의 수가 크게 증가하지 않았다는 것을 확인할 수 있다.
<br><br>

<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/78bef296-cfcf-4134-8bde-cf1cc1361608">

  <br>  
    
  출처 [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION PAPER](https://arxiv.org/pdf/1409.1556v6.pdf)
  </p>
</div>

<br><br>

## 학습
VGG의 학습 과정은 학습 시에 다양한 크기의 입력 이미지들을 크롭하여 샘플링 하는 방법을 사용하는 것을 제외하면 AlexNet과 동일하고, 사용한 하이퍼파라미터는 다음과 같다.

- Optimizer: 미니배치 경사하강법
- Loss function: 다항 로지스틱 회귀
- batch_size = 256, momentum = 0.9, weight_decay = L2, 0.0005
- 1,2 번째 FC 계층에서의 dropout = 0.5
- 초기 학습률 = 0.01(개선되지 않으면 10배로 감소)

위의 하이퍼파라미터를 사용한 결과, 학습률이 총 3번 줄어들고 74에포크 동안 학습이 진행됐다. AlexNet에 비해 파라미터와 더 많고 깊이가 더 깊었는데도 작은 합성곱 필터크기와 regularization 등의 효과 덕분에 에포크가 많이 필요하지 않았다.

네트워크에서 가중치를 초기화 하는 것은 학습의 안정성 측면에서 매우 중요한데 저자는 깊이가 얕은 A 아키텍처를 랜덤으로 초기화한 가중치를 더 복잡한 아키텍처들의 1~4번째 convolutional 층과 마지막 3개의 FC층의 가중치를 초기화하는 용도로 사용했다. 랜덤 초기화는 가능하다면 평균이 0, 분산이 0.01인 정규분포로 초기화 했고 편향은 0으로 초기화했다. 여기서 중요한 점은 논문을 제출한 후에 알게 된 사실이지만 [Glorot & Bengio의 2010년 논문](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)에 따르면 사전훈련 없이 가중치를 초기화하는 방법이 존재했다.

학습에 사용할 224 x 224 사이즈의 이미지를 만들기 위해서는 다양한 크기의 input 이미지를 crop 해야한다. crop은 AlexNet과 같이 좌우반전과 RGB 색상변환을 사용했고 사이즈에 대해서는 3가지 방법을 통해 crop을 진행했다.
- input 이미지를 256 x 256 사이즈로 고정(single-scale model)
- input 이미지를 384 x 384 사이즈로 고정(single-scale model)
- input 이미지를 \[256, 512] 범위에서 랜덤하게 resize(multi-scale model)

위와 같은 방법을 사용하여 다양한 크기의 이미지가 입력되어도 학습이 잘 이루어지게 했고, 특히 3번째 방법의 경우 사전에 학습한 2번째 방법의 모든 레이어를 Fine tuning하여 학습시간을 줄였다.

<br><br>

## 검증
검증에서도 학습과 마찬가지로 고정된 크기의 입력 이미지를 사용했는데 검증 이미지의 크기가 꼭 학습 이미지의 크기와 같을 필요는 없고, 오히려 학습 이미지와 다른 크기의 이미지를 사용했을 때 성능이 더 좋아졌다.
검증 과정에서는 과적합을 방지하기 위해 기존의 FC계층을 convolutional 층(첫번째 FC층은 7x7, 두세번째 층은 1x1)으로 수정하여 기존에 Fully Connected Layer가 Fully Convolutional Layer라고 불리기도 한다. FC 계층을 통과한 출력값은 각 클래스에 속할 점수맵으로 이루어져있고 채널의 수는 예측하고자 하는 Class의 수와 동일하다. 

검증 이미지에도 학습과 마찬가지로 데이터 증강을 적용하게되는데 원본 이미지와 증강 이미지에 대한 클래스들의 점수맵을 구한 후 soft-max에 평균을 취해 하나의 이미지에 대한 클래스별 점수맵을 얻는다. 최종적으로는 고정된 벡터를 얻기 위해 class score map에 spatially averaged를 취한다. Spatially averaged는 공간적으로 평균을 구한다는 의미로 한 위치에서 여러개의 클래스에 속할 확률을 평균화하여 하나의 클래스에 대한 확률값으로 변환시키는 것을 말한다. 

Fully Convolutional Network를 사용하여 이미 이미지내의 전체 영역을 사용할 수 있기때문에 검증 과정에서 여러번 이루어지는 crop은 연산량의 측면에서 불필요해졌다.

## GPU
Multi GPU를 사용하여 여러개의 이미지 배치를 각 GPU마다 할당했고 각각의 GPU에서 나온 기울기들을 평균 내어 전체 이미지 배치의 기울기를 구했다. 그 결과, 하나의 GPU를 사용했을 때와의 결과가 일치했다. 4개의 GPU를 사용했을 때 단일 GPU보다 약 3.75배 빠른 속도를 보였고 아키텍처에 따라 학습 시간이 2~3주 정도 소요됐다.

## 평가
**Single Scale Evaluation**

테스트 데이터에 대한 Single scale evaluation에서는 학습 이미지를 256 or 384로 고정된 이미지를 사용하는 것보다 \[256,512] 범위로 랜덤하게 resizing한 이미지를 사용하는 것이 더 성능이 좋았다. 이를 통해 scale jittering을 통한 학습 데이터 증강이 multi scale의 이미지 정보를 파악하기 좋다는 것을 알 수 있다. 성능은 아래의 표와 같이 층이 깊을수록 뛰어났고, A-LRN이 A보다 에러율이 높을 것을 보다 Local Reponse Normalization이 큰 도움이 되지 않다는 것을 확인할 수 있다.

<br><br>

<div align="center">
  <p>
  <img width="500" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/a435c8ac-ff68-4170-95a5-eb5231126e8f">

  <br>  
    
  출처 [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION PAPER](https://arxiv.org/pdf/1409.1556v6.pdf)
  </p>
</div>

**Multi-Scale Evaluation**

Multi scale evaluation에서는 다양한 스케일의 테스트 이미지를 평균내어 성능을 평가했다. 그 결과, 테스트 이미지에 대해 single scale evaluation과 같이 스케일을 고정시키는 것보다 scale jittering을 적용시켰을 때 성능이 더 좋았고 역시 층의 깊이가 깊을수록 에러율이 더 낮았다. Single scale evaluation보다 더 개선된 성능이다.


<br><br>

<div align="center">
  <p>
  <img width="530" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/59423cae-5c1a-45ae-8cbd-8fffdcb4ca11">

  <br>  
    
  출처 [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION PAPER](https://arxiv.org/pdf/1409.1556v6.pdf)
  </p>
</div>



**Multi-Crop Evaluation**
Multi crop evaluation에서는 원본 이미지에 대한 dense evaluation과 증강 이미지에 대한 multi-crop evaluation의 soft-max output을 평균화해서 성능을 평가했다.
Multi crop evaluation이 조금 더 성능이 좋지만, 연산량이 dense evaluation보다 크기때문에 적절히 조화시켜 평균을 내는 방법이 가장 좋았다. 이전 결과와 마찬가지로 층이 깊을수록 작은 에러율을 보였다.

<br><br>

<div align="center">
  <p>
  <img width="477" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/71f061c5-1301-462b-a349-48ff6271d710">

  <br>  
    
  출처 [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION PAPER](https://arxiv.org/pdf/1409.1556v6.pdf)
  </p>
</div>

<br>

그 이후 여러가지 모델을 조합하여 성능을 향상 시켰고, 최종적으로는 아래의 표와 같이 ILSVRC classification에서 2등으로 입상했고, Single Net 관점에서는 GoogLeNet보다 0.9% 낮은 에러율을 보였다.

<div align="center">
  <p>
  <img width="560" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/5e8c777b-4397-43dd-b5df-385d63f618f1">

  <br>  
    
  출처 [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION PAPER](https://arxiv.org/pdf/1409.1556v6.pdf)
  </p>
</div>

<br><br>

## 결론
층의 깊이가 깊을수록 분류의 정확도에 긍정적인 영향을 끼치는 것을 확인했고, 기존 Convolutional Network 아키텍처의 큰 변화없이 깊이만 증가시켜 좋은 성능을 낼 수 없음을 보였다.

<br><br>

# 개인적인 생각
- *LeCun의 1989년 논문의 고전 ConvNet 아키텍처를 벗어난 것이 아닌, 단순히 깊이를 더하는 것으로 점진적으로 개선했을 뿐이다.

<br><br>

# 구현

```python
        
        
```

