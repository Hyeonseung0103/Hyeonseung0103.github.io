---
layout: single
title: "Section1-Sprint1-EDA"
categories:
  - Section1
---

AI 캠프를 시작한지 일주일이 지났다.
R에서 배웠던 전처리 코드들을 파이썬 pandas 패키지를 사용해서 써보았다.
크게 어렵지 않아서, 잘 안 외워지거나 새로 알게된 내용들만 정리해보자.
코드보고 싶으면 github가서 직접 보기.
note에 없으면 dicussion 링크가서 보기.

## Note111
### 엑셀 파일 불러오기
특정시트 불러오려면 저렇게 시트이름 명시해주면 됨.
```python
pd.read_excel('url..........' ,sheet_name = ' ')
```

### 칼럼이름이 이름으로 안 가 있고, 1행에 가있을때
```python
new_header = df1.iloc[0]
df1 = df1[1:] 
df1.columns = new_header
```

### 통계값 확인
그 중에 numeric 변수 통계값 확인
```python
print(pp.describe(include = 'number'))
```
## Note112-EDA
### 숫자 천 단위에 ,있는 변수있으면 int로 바꿀 때 이 함수 쓰면 편할듯
apply로 적용할 것. apply 안에는 함수가 들어가야한다.

```python
def toInt(string):
    return int(string.replace(',',''))

df['변수'] = df['변수'].apply(toInt)
```

### 원하는 type의 변수만 보기
include는 해당타입포함한 df, exclude는 해당타입제외 df
columns붙이면 그 변수 이름들만 추출할 수 있겠지
```python
df.select_dtypes(include = 'object')
```

### numeric이지만 천단위 comma 땜에 object 타입으로 저장되어있는 있는 애들 한번에 바꾸기
그냥 for문 쓰는게 젤 빠를듯...

```python
col = df.select_dtypes(include = 'object').columns
for i in range(len(col)):
  df[col[i]] = df[col[i]].apply(to_int)
```

### NA 집어넣기
특정 위치에 집어넣음. index 모르면 그냥 loc해서 원하는 곳 필터해서 넣으면 될 듯.
```python
import numpy as np
df['변수'][2] = np.NaN
```

### 원하는 구간에 값 넣기
bin을 quantile로 나누는 방법도 있겠지만 내가 직접 넣을 수도 있음.

```python
df.loc[df['매출액'] >= np.mean(df['매출액'])*1.1, "Relative_Performance"] = "S"
df.loc[(df['매출액'] >= np.mean(df['매출액'])*1.05) & (df['매출액'] < np.mean(df['매출액']) * 1.1), "Relative_Performance"] = "A"
df.loc[(df['매출액'] >= np.mean(df['매출액'])*0.95) & (df['매출액'] < np.mean(df['매출액']) * 1.05), "Relative_Performance"] = "B"
df.loc[(df['매출액'] >= np.mean(df['매출액'])*0.9) & (df['매출액'] < np.mean(df['매출액']) * 0.95), "Relative_Performance"] = "C"
df.loc[df['매출액'] < np.mean(df['매출액']) * 0.9, "Relative_Performance"] = "D"
```

## Note113-Data manipulation
### melt, merge, concat 이런거 잘 안 되면 index 리셋하거나 해서 다시해봐라. index가 문제일 수도 있음.

### 원하는 칼럼만 이름 바꾸기
```python
df.rename(columns = {'원래이름' : '새이름'})
```

### 원하는 칼럼 드롭
```python
df.drop('변수', axis=1)
```

### NA가 몇개 이상 있는행 드롭
```python
#na가 3개 이상인 행 드롭
#열로 할 거면 axis = 1인가 하면 됐었나...잘 기억 안 남.
df.dropna(thresh = 3)
```

### 원하는 변수만 추출 
```python
df[['변수1', '변수2', '변수3']]
```

### isin
isin은 series에만 적용할 수 있음. 그러니까 변수에만
```python
df[df['변수'].isin(['어쩌고저쩌고'])] 
```
not isin은 앞에 ~ 붙이면 됨
```python
df[~df['변수'].isin(['어쩌고저쩌고'])]
```

### melt, pivot_table
gather는 melt, spread는 pivot_table
```python
df.melt(id_vars = '기준', value_vars = ['바꿀 변수1', '바꿀 변수2'])
df.pivot_table(index = '기준', columns = '펼칠 변수가 담겨있는 변수', values = '값')
```

### groupby와 agg 써서 여러개 통계값 표현
agg안 쓰면 통계값 하나만 표시하고 to_frame()하면 됨.
```python
df.groupby('passenger_class')['fare'].agg(['mean','var','max','min'])
df.groupby('그룹')['통계값 나타낼 변수'].mean().to_frame()
```

### Categorical 변수 level 변환
```python
#굳이 apply 안 쓰고 이렇게 바꿔도됨.
df['class'] = df['class'].cat.rename_categories({'01' : 'First','02' : 'Second','03' : 'Third'})
```

## Note114-미분
### 미분이 데이터 사이언스에서 중요한 이유
회귀 모델을 예시로들면 우리가 모델을 돌리고 예측을 하고나면 실제값과 예측값의 차이 즉, 잔차가 발생한다. 이 잔차를 최소화 하는 방법은 순간 변화율이 0이 되는 부분(기울기가 0)을 찾는 것이다.
이를 위해 편미분, Chain rule과 같은 미분 기법을 사용하고, 실사용 예시로는 경사하강법이 있다.

### 편미분
파라미터가 2개 이상인 Error 함수에서 우선 1개의 파라미터에 대해서만 미분을 하자 라는 목적으로 다른 변수들을 상수 취급 하는 방법을 말한다.

![image](https://user-images.githubusercontent.com/97672187/154600254-8ff4bf96-8c0b-414b-b3d4-c072132ce42c.png)

![image](https://user-images.githubusercontent.com/97672187/154600329-3e48e275-9f41-4b44-bfd4-0184fff00b37.png)

![image](https://user-images.githubusercontent.com/97672187/154600288-0d3f3c7d-223c-4cc7-ba2a-a1be85ae84d2.png)



### Chain rule
합성함수의 미분. Deep learning의 back propagarion에서 활용. 
양파까는 것을 생각하면 된다. 안에 걸 하나의 x라 생각하고 밖에꺼 먼저 미분하고, 이거 곱하기 안에꺼 미분한 거.

![image](https://user-images.githubusercontent.com/97672187/154600371-8a26702d-c77c-4cc7-9b15-3945f1930348.png)

![image](https://user-images.githubusercontent.com/97672187/154600404-530605ae-a1c1-4880-b286-1748321fea33.png)

![image](https://user-images.githubusercontent.com/97672187/154600742-f20a4b3b-272d-4c2c-94c2-d84043bd34d1.png)

![image](https://user-images.githubusercontent.com/97672187/154600438-ce556f92-0282-45ae-b126-595762b2c636.png)

### 경사하강법
목표: 오차 함수를 최소화 시키기(기울기가 0)  
한계: 수학적으로 오차 함수가 아예 0이 되는 것은 불가능함
목표조정: 오차 함수를 0에 가장 가깝게 하는 파라미터를 찾자!
예시 note114 함수 쉽게 잘 나와있는 듯. 보고 이해하기

Gradient descent는 경사의 반대 방향으로 step size만큼 움직이면서 기울기가 최소가 되는 지점을 찾는다. 이 기울기는 loss이기 때문에 기울기가 최소가 되는 구간이 가장 정확한(예측을 잘하는) 모델을 만드는데 도움이 된다. Convex function에서는 전역의 최솟값을 찾을 수 있지만, not convex function에서는 지역의 최솟값을 찾기 때문에 이 경사하강법에서 도출한 결과가 최적의 해라고 확신할 수 없다. 왜냐면 그 지역의 최솟값이 전체의 최솟값인 줄 알고 해당 최솟값에 갇힐 수 있기 때문에. 또한, 기울기를 낮추며 이동할 때 얼만큼 이동할 것인가를 step size로 결정할 수 있는데 이 step size가 너무 작으면 최솟값을 찾는데 너무 많은 시간이 소요되고, step size가 너무 크면 최솟값을 그냥 지나쳐서(경사가 바뀐걸 인지하면 돌아감) 다시 최솟값을 찾는 과정이 반복되어 이 또한 많은 시간이 소요된다. 따라서, 이 step size를 얼만큼으로 하는가가 경사 하강법에서 매우 중요한 포인트다.
