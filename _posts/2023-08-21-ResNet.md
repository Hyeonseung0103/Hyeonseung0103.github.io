---
layout: single
title: "ResNet(Deep Residual Learning for Image Recognition)논문 요약"
toc: true
toc_sticky: true
category: CNN
---

# Abstract
신경망의 깊이가 깊어질수록 학습이 더 어려워진다. 본 논문에서는 레이어를 잔차함수로 변환하는 residual learning framework를 적용하여 이전의 모델들보다 더 깊은 신경망일지라도 학습이 잘 되게 했다.  
깊이가 최대 152인 네투워크이지만 VGG모델보다 8배 깊지만 더 낮은 복잡성을 가지고 ImageNet 데이터셋에서 ILSVRC 2015 Classification 부문에서 1위를 차지했다.

<br><br>

# Details
## 도입부 & 관련 논문
- 여러 연구를 통해 깊이가 깊어질수록 딥러닝 모델의 성능이 더 좋아진다는 것을 알 수 있었다.
- 깊이의 중요성이 부각되면서 더 좋은 네트워크를 구축하기 위해 단순히 깊이만 증가시키면 되냐는 질문을 할 수 있는데 깊이가 깊어질수록 vanishing/exploding gradients 문제가 발생할 수 있고,
가중치 초기화나 배치 정규화 등을 통해 이 문제를 해결할 수 있었다.

<br><br>

<div align="center">
  <p>
  <img width="490" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/0d9eecf7-d684-43ee-a10f-f2657017fabe">
  </p>
</div>

<br><br>

- 하지만, 깊이가 깊어질수록 위의 그래프처럼 정확도가 포화상태가 되는 현상도 발생할 수 있는데 과대적합과는 별개로 깊이때문에 training error가 크게 발생할 수 있다.
- 얕은 모델에 레이어를 더해 보다 깊은 모델을 만들었으면 이 모델은 기존의 얕은 모델보다 더 높은 정확도가 되어야하는데 그렇지 못한 경우가 생긴다. 본 논문은 이러한 문제를 해결하기 위해 residual mapping을 적용한다.
- residual mapping은 입력으로부터 출력을 예측하는 것이 아니라 입력이 출력이 되기 위한 가중치 layer를 예측하는 방법으로 아래와 같이 표현할 수 있다.

<br><br>

<div align="center">
  <p>
  <img width="288" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/55587f09-54f1-44b9-819f-9c598652d3fc">

  </p>
</div>

<br><br>

- 예측하고자 하는 output인 H(x), input을 x, 가중치 layer를 F(x)라고 하면, H(x) = F(x) + x라고 할 수 있고, output인 H(x)를 도출하기 위해 F(x)를 학습하는 것이다. F(x) = H(x) - x 라고 정의할 수 있고 이는 결국 output과 input의 차이를 학습하는 것이기때문에 residual mapping이라고 표현한다.
- 극단적으로 identity mapping이 최적이라고 할 때 여러 개의 비선형 레이어를 쌓아 identity mapping에 fit하게 만드는 것보다 잔차를 0으로 만드는 방법이 더 쉬운 방법이다.
- F(x)에 x를 더하는 방법을 shortcut connections이라고 표현하고(skip connections이라고 부르기도 함) x를 더해줌으로써 input과 output의 크기가 같아지기때문에 identity mapping의 효과도 적용된다. 복잡한 연산이 사용되지 않고 단순히 가중치 레이어에 입력값을 더하는 해이기때문에 시간 복잡도나 파라미터수가 크게 요구되지 않아서 결과적으로 깊이가 깊어지더라도 기존의 네트워크들보다 적은 연산으로 성능을 올릴 수 있는 방법이 됐다.
- VLAD 논문에서 이미 잔차 벡터를 인코딩한 vector quantization 방법이 효과적이라는 연구가 있었고, GoogLeNet에서는 기울기 소실/폭발 문제를 해결하기위해 auxiliary classifier를 활용해서 중간층에서도 출력값을 만들었다. 또한, highway networks 논문에서는 LSTM처럼 gate의 개념을 활용해서 residual을 적용시켰는데 본 논문은 gate가 닫히면(gate에 대한 입력이 0일 때) residual functions를 활용하지않은 highway networks와는 달리 언제나 residual functions을 사용하여 학습한다는 점에서 차이가 존재한다.

<br><br>



## Deep Residual Learning
### 1. Residual Learning

### 2. Identity Mapping by Shortcuts

### 3. Network Architectures

### 5. Implementation

<br><br>

<div align="center">
  <p>
  <img width="490" alt="image" src="https://github.com/Hyeonseung0103/Hyeonseung0103.github.io/assets/97672187/0d9eecf7-d684-43ee-a10f-f2657017fabe">
  </p>
</div>

<br><br>



<br><br>

## Experiments
### 1. ImageNet Classification



### 2. CIFAR-10 and Analysis

### 3. Object Detection on PASCAL and MS COCO


### 3. 훈련 및 성능
**훈련 방법**


**성능**


<br><br>


## 결론


<br><br>

# 개인적인 생각
- 

<br><br>

# 구현
ResNet을 pytorch로 구현해보자([참고]([https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/googlenet.py](https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py)https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py).

```python

```


```python


```

# 이미지 출처
- 
